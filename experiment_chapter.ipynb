{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99078157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import tokenize\n",
    "import LRDEstimator\n",
    "import utils\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as sps\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73232068",
   "metadata": {},
   "source": [
    "## Standardized Project Gutenberg Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf6885",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ea384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sampled dataset\n",
    "spgc_metadata_sampled = pd.read_csv(\"data/spgc_metadata_sampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce4a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be calculating Coco for pooled embeddings of order 1, 3, 9 and 27\n",
    "# Then we will be fitting power law (gamma) and stretched exponential function (delta and beta)\n",
    "# To our dataframe we need to add columns for storing the results\n",
    "new_columns = [\n",
    "    \"gamma_1\", \"gamma_3\", \"gamma_9\", \"gamma_27\",\n",
    "    \"delta_1\", \"delta_3\", \"delta_9\", \"delta_27\",\n",
    "    \"beta_1\", \"beta_3\", \"beta_9\", \"beta_27\",\n",
    "]\n",
    "spgc_metadata_sampled[new_columns] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c40e879",
   "metadata": {},
   "source": [
    "### Define reusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec_embeddings(language_code: str, base_path: str = \"embeddings\") -> KeyedVectors:\n",
    "    \"\"\"\n",
    "    Load word2vec embeddings for a given language code.\n",
    "    \"\"\"\n",
    "    model_path = f\"{base_path}/word2vec_{language_code}.bin\"\n",
    "    return KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(file_name: str, base_path: str = \"data/SGPC\") -> list:\n",
    "    \"\"\"\n",
    "    Load tokens from a file and return them as a list.\n",
    "    \"\"\"\n",
    "    file_path = f\"{base_path}/{file_name}_tokens.txt\"\n",
    "    with open(file_path, mode=\"r\", encoding=\"UTF-8\") as f:\n",
    "        tokens = f.read().split(\"\\n\")\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce296431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coco_values_for_book(vectors: np.ndarray, \n",
    "                                 file_name: str,\n",
    "                                 language: str,\n",
    "                                 pool_order: int,\n",
    "                                 coco_results: list,\n",
    "                                 lag_growth_factor: float = 1.1):\n",
    "    \"\"\"\n",
    "    Given the vectors for a single book, compute CoCo values at various lags\n",
    "    and store them in the coco_results list of dicts.\n",
    "    \"\"\"\n",
    "    lrd = LRDEstimator.LRDEstimator(vectors)\n",
    "    max_lag = int(vectors.shape[0] / 2) # TBD: maybe this should be smaller for poolend embeddings?\n",
    "    current_lag = 1\n",
    "    \n",
    "    while current_lag < max_lag:\n",
    "        # Example without permutation test:\n",
    "        coco_value = lrd.calculate_coco(lag=current_lag, pool_order=pool_order)\n",
    "        \n",
    "        coco_results.append({\n",
    "            \"language\": language,\n",
    "            \"book_id\": file_name,\n",
    "            \"pool_order\": pool_order,\n",
    "            \"lag\": int(current_lag),\n",
    "            \"coco_value\": coco_value,\n",
    "        })\n",
    "        \n",
    "        # Increase lag by ~10%\n",
    "        current_lag = int(np.ceil(current_lag * lag_growth_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_store_curves(spgc_df, coco_results, file_name, pool_order):\n",
    "    \"\"\"\n",
    "    Fit power law and stretched exponential to the CoCo results for a single\n",
    "    book_id and pool_order. Store results back into spgc_df.\n",
    "    \"\"\"\n",
    "    # Filter the relevant CoCo results\n",
    "    book_data = [\n",
    "        r for r in coco_results \n",
    "        if (r[\"book_id\"] == file_name) and (r[\"pool_order\"] == pool_order)\n",
    "    ]\n",
    "    if not book_data:\n",
    "        return  # No data to fit\n",
    "    \n",
    "    lags = np.array([d[\"lag\"] for d in book_data])\n",
    "    c_coco = np.array([d[\"coco_value\"] for d in book_data])\n",
    "    abs_coco = np.abs(c_coco)\n",
    "    \n",
    "    # Fit power law\n",
    "    try:\n",
    "        popt_pl, _ = curve_fit(\n",
    "            utils.power_law,\n",
    "            lags,\n",
    "            abs_coco,\n",
    "            bounds=([-np.inf, -np.inf], [np.inf, 0]),\n",
    "            maxfev=5000\n",
    "        )\n",
    "        gamma_col = f\"gamma_{pool_order}\"\n",
    "        spgc_df.loc[spgc_df[\"id\"] == file_name, gamma_col] = popt_pl[1]\n",
    "    except RuntimeError:\n",
    "        pass  # If fitting fails, leave as NaN\n",
    "    \n",
    "    # Fit stretched exponential\n",
    "    try:\n",
    "        popt_se, _ = curve_fit(\n",
    "            utils.stretched_exponential,\n",
    "            lags,\n",
    "            abs_coco,\n",
    "            bounds=([0, 0, -np.inf], [np.inf, 1, np.inf]),\n",
    "            maxfev=5000\n",
    "        )\n",
    "        delta_col = f\"delta_{pool_order}\"\n",
    "        beta_col = f\"beta_{pool_order}\"\n",
    "        spgc_df.loc[spgc_df[\"id\"] == file_name, delta_col] = popt_se[0]\n",
    "        spgc_df.loc[spgc_df[\"id\"] == file_name, beta_col] = popt_se[1]\n",
    "    except RuntimeError:\n",
    "        pass  # If fitting fails, leave as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3731b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_language(spgc_df, language, pool_orders, coco_results):\n",
    "    \"\"\"\n",
    "    Process all books for a single language and multiple pool_orders.\n",
    "    \"\"\"\n",
    "    print(f\"Processing language: {language}\")\n",
    "    language_code = re.findall(r\"[a-z]{2}\", language)[0]\n",
    "    \n",
    "    # Filter metadata for the current language\n",
    "    spgc_metadata_current = spgc_df[spgc_df[\"language\"] == language]\n",
    "    \n",
    "    # Load embeddings once per language\n",
    "    model_current = load_word2vec_embeddings(language_code)\n",
    "    \n",
    "    # Iterate through each book in the current language\n",
    "    for index, row in spgc_metadata_current.iterrows():\n",
    "        file_name = row[\"id\"]\n",
    "        \n",
    "        # Load tokens\n",
    "        tokens = load_tokens(file_name)\n",
    "        \n",
    "        # Build embeddings\n",
    "        vectors = np.asarray([model_current[w] for w in tokens if w in model_current])\n",
    "        \n",
    "        # Skip if not enough vectors\n",
    "        if len(vectors) < 2:\n",
    "            continue\n",
    "        \n",
    "        # For each pool_order, compute CoCo values\n",
    "        for p_order in pool_orders:\n",
    "            compute_coco_values_for_book(\n",
    "                vectors=vectors,\n",
    "                file_name=file_name,\n",
    "                language=language,\n",
    "                pool_order=p_order,\n",
    "                coco_results=coco_results\n",
    "            )\n",
    "        \n",
    "        # For each pool_order, fit the curves and store\n",
    "        for p_order in pool_orders:\n",
    "            fit_and_store_curves(spgc_df, coco_results, file_name, p_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a35b6c",
   "metadata": {},
   "source": [
    "### Run calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_coco_pipeline(spgc_metadata_sampled):\n",
    "    \"\"\"\n",
    "    Main pipeline function that:\n",
    "      1) Determines unique languages,\n",
    "      2) Iterates over each language,\n",
    "      3) Computes CoCo for each pool_order,\n",
    "      4) Fits curves, and\n",
    "      5) Stores results.\n",
    "    \"\"\"\n",
    "    languages = spgc_metadata_sampled[\"language\"].unique()\n",
    "    pool_orders = [0, 3, 9, 27]\n",
    "    \n",
    "    # We'll collect results in this list of dictionaries\n",
    "    coco_results_records = []\n",
    "    \n",
    "    for language in languages:\n",
    "        process_language(\n",
    "            spgc_df=spgc_metadata_sampled,\n",
    "            language=language,\n",
    "            pool_orders=pool_orders,\n",
    "            coco_results=coco_results_records\n",
    "        )\n",
    "    \n",
    "    return spgc_metadata_sampled, coco_results_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac19821",
   "metadata": {},
   "outputs": [],
   "source": [
    "spgc_metadata_sampled = pd.read_csv(\"data/spgc_metadata_sampled.csv\")\n",
    "spgc_metadata_sampled, coco_results = main_coco_pipeline(spgc_metadata_sampled)\\\n",
    "\n",
    "# It takes ~2 hours to complete\n",
    "\n",
    "# Save the results to a CSV file\n",
    "spgc_metadata_sampled.to_csv(\"results/spgc_metadata_sampled_after.csv\", index=False)\n",
    "coco_results_df = pd.DataFrame(coco_results)\n",
    "coco_results_df.to_csv(\"results/coco_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06a387",
   "metadata": {},
   "source": [
    "### Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a9a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results for further analysis\n",
    "spgc_metadata_sampled = pd.read_csv(\"results/spgc_metadata_sampled_after.csv\")\n",
    "\n",
    "# Explore missing values in the fitted parameters\n",
    "selected_columns = spgc_metadata_sampled.filter(regex=\"^(gamma|delta|beta)\").columns\n",
    "missing_values = spgc_metadata_sampled[selected_columns].isnull().sum()\n",
    "missing_percentage = (missing_values / len(spgc_metadata_sampled)) * 100\n",
    "\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    \"Parameter Name\": missing_values.index,\n",
    "    \"Missing Count\": missing_values.values,\n",
    "    \"Missing Percentage\": missing_percentage.values\n",
    "})\n",
    "\n",
    "missing_data_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b119dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now skip Chinese language, because there were problems with the embeddings\n",
    "spgc_metadata_sampled = spgc_metadata_sampled[spgc_metadata_sampled[\"language\"] != \"['zh']\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba57e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse sign of gamma\n",
    "spgc_metadata_sampled[\"gamma_1\"] = -spgc_metadata_sampled[\"gamma_1\"]\n",
    "spgc_metadata_sampled[\"gamma_3\"] = -spgc_metadata_sampled[\"gamma_3\"]\n",
    "spgc_metadata_sampled[\"gamma_9\"] = -spgc_metadata_sampled[\"gamma_9\"]\n",
    "spgc_metadata_sampled[\"gamma_27\"] = -spgc_metadata_sampled[\"gamma_27\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcad066",
   "metadata": {},
   "source": [
    "#### Distribution of fitted parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b61e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip brackets from language codes\n",
    "spgc_metadata_sampled[\"language\"] = spgc_metadata_sampled[\"language\"].str.replace(r\"[\\[\\]']\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f561861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boxplot(df, col_value, col_category, title):\n",
    "    plot_data = []\n",
    "    groups = []\n",
    "\n",
    "    for group in df[col_category].unique():\n",
    "        group_data = df[df[col_category] == group][col_value].dropna()\n",
    "        plot_data.append(group_data)\n",
    "        groups.append(group)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    bp = ax.boxplot(plot_data, patch_artist=True, showfliers=False)\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(plot_data)))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color) \n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticklabels(groups, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Value')\n",
    "        \n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = [\n",
    "    \"gamma_1\", \"gamma_3\", \"gamma_9\", \"gamma_27\",\n",
    "    \"delta_1\", \"delta_3\", \"delta_9\", \"delta_27\",\n",
    "    \"beta_1\", \"beta_3\", \"beta_9\", \"beta_27\",\n",
    "]\n",
    "\n",
    "for param in all_params:\n",
    "    # Titles with greek letters\n",
    "    if param.startswith(\"gamma\"):\n",
    "        title = r\"Fitted $\\gamma$ (order {}) for all languages\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"delta\"):\n",
    "        title = r\"Fitted $\\delta$ (order {}) for all languages\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"beta\"):\n",
    "        title = r\"Fitted $\\beta$ (order {}) for all languages\".format(param.split(\"_\")[1])\n",
    "    else:\n",
    "        title = f\"Fitted {param} for all languages\"\n",
    "\n",
    "    create_boxplot(\n",
    "        df = spgc_metadata_sampled,\n",
    "        col_value = param,\n",
    "        col_category = \"language\",\n",
    "        title = title\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f326ba99",
   "metadata": {},
   "source": [
    "#### Analysis of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a76d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store the Kruskal-Wallis test results\n",
    "kw_results = pd.DataFrame(\n",
    "    index=['gamma', 'delta', 'beta'],\n",
    "    columns=['1', '3', '9', '27']\n",
    ")\n",
    "\n",
    "# Define parameter groups\n",
    "param_groups = {\n",
    "    'gamma': ['gamma_1', 'gamma_3', 'gamma_9', 'gamma_27'],\n",
    "    'delta': ['delta_1', 'delta_3', 'delta_9', 'delta_27'],\n",
    "    'beta': ['beta_1', 'beta_3', 'beta_9', 'beta_27']\n",
    "}\n",
    "\n",
    "# Perform Kruskal-Wallis test for each parameter\n",
    "for group_name, group_params in param_groups.items():\n",
    "    for param in group_params:\n",
    "        pool_order = param.split('_')[1]  # Extract pool order (1, 3, 9, 27)\n",
    "        \n",
    "        # Create a list of data for each language\n",
    "        groups = []\n",
    "        group_labels = []\n",
    "        \n",
    "        for lang in spgc_metadata_sampled['language'].unique():\n",
    "            data = spgc_metadata_sampled.loc[spgc_metadata_sampled['language'] == lang, param].dropna()\n",
    "            if len(data) > 0:\n",
    "                groups.append(data)\n",
    "                group_labels.append(lang)\n",
    "        \n",
    "        # Perform Kruskal-Wallis test\n",
    "        if len(groups) > 1:  # Need at least 2 groups for the test\n",
    "            stat, p_value = sps.kruskal(*groups)\n",
    "            kw_results.loc[group_name, pool_order] = p_value\n",
    "        else:\n",
    "            kw_results.loc[group_name, pool_order] = float('nan')\n",
    "\n",
    "# Format p-values with scientific notation for small values\n",
    "kw_results_formatted = kw_results.applymap(lambda x: f\"{x:.2e}\" if pd.notnull(x) else \"NaN\")\n",
    "\n",
    "# Display the results\n",
    "kw_results_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform post-hoc Dunn test with Bonferroni correction for each parameter\n",
    "# We'll create a function to run the test and visualize the results\n",
    "\n",
    "def perform_dunn_test_and_visualize(parameter_groups, spgc_metadata_sampled):\n",
    "    \"\"\"\n",
    "    Perform Dunn's post-hoc test with Bonferroni correction for each parameter group\n",
    "    and visualize the results using heatmaps.\n",
    "    \n",
    "    Args:\n",
    "        parameter_groups: Dictionary of parameter groups to analyze\n",
    "        spgc_metadata_sampled: DataFrame containing the data\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "    fig.suptitle('Dunn Test Results (p-values with Bonferroni correction)', fontsize=18)\n",
    "    \n",
    "    # Define symbols for parameter groups for better visualization\n",
    "    param_symbols = {\n",
    "        'gamma': '$\\gamma$',\n",
    "        'delta': '$\\delta$',\n",
    "        'beta': '$\\\\beta$'\n",
    "    }\n",
    "    \n",
    "    # Loop through parameter groups\n",
    "    for row, (group_name, params) in enumerate(parameter_groups.items()):\n",
    "        param_symbol = param_symbols.get(group_name, group_name)\n",
    "        \n",
    "        for col, param in enumerate(params):\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Extract language and data for Dunn test\n",
    "            groups = []\n",
    "            lang_codes = []\n",
    "            \n",
    "            for lang in spgc_metadata_sampled['language'].unique():\n",
    "                data = spgc_metadata_sampled.loc[spgc_metadata_sampled['language'] == lang, param].dropna()\n",
    "                if len(data) > 0:\n",
    "                    groups.append(data.values)\n",
    "                    lang_code = lang.strip(\"[]'\")\n",
    "                    lang_codes.append(lang_code)\n",
    "            \n",
    "            # Perform Dunn test\n",
    "            if len(groups) > 2:  # Need at least 3 groups for post-hoc\n",
    "                try:\n",
    "                    # Perform Dunn test with Bonferroni correction\n",
    "                    posthoc_results = sp.posthoc_dunn(groups, p_adjust='bonferroni')\n",
    "                    \n",
    "                    # Create a heatmap for the p-values\n",
    "                    sns.heatmap(posthoc_results, annot=True, cmap='coolwarm_r', \n",
    "                               vmin=0, vmax=0.05, ax=ax, xticklabels=lang_codes, \n",
    "                               yticklabels=lang_codes, cbar=False, fmt='.3f')\n",
    "                    \n",
    "                    # Color p-values < 0.05 differently for emphasis\n",
    "                    for i in range(posthoc_results.shape[0]):\n",
    "                        for j in range(posthoc_results.shape[1]):\n",
    "                            if posthoc_results.iloc[i, j] < 0.05:\n",
    "                                ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=False, edgecolor='black', lw=1.5))\n",
    "                    \n",
    "                    # Set plot title and labels\n",
    "                    ax.set_title(f\"{param_symbol}_{param.split('_')[1]}\")\n",
    "                except Exception as e:\n",
    "                    ax.text(0.5, 0.5, f\"Error: {str(e)}\", horizontalalignment='center',\n",
    "                           verticalalignment='center', transform=ax.transAxes)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, \"Insufficient data for\\npost-hoc test\", horizontalalignment='center',\n",
    "                       verticalalignment='center', transform=ax.transAxes)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Run the post-hoc tests and visualize\n",
    "perform_dunn_test_and_visualize(param_groups, spgc_metadata_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74456364",
   "metadata": {},
   "source": [
    "## Human vs LLM Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251b381",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sampled dataset\n",
    "df_human_vs_llm_sampled = pd.read_csv(\"data/human_vs_llm_sampled.csv\")\n",
    "\n",
    "new_columns = [\n",
    "    \"gamma_1\", \"gamma_3\", \"gamma_9\", \"gamma_27\",\n",
    "    \"delta_1\", \"delta_3\", \"delta_9\", \"delta_27\",\n",
    "    \"beta_1\", \"beta_3\", \"beta_9\", \"beta_27\",\n",
    "]\n",
    "df_human_vs_llm_sampled[new_columns] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"embeddings/word2vec_en.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d978d",
   "metadata": {},
   "source": [
    "### Run calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_orders = [1, 3, 9, 27]\n",
    "coco_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78465b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Runtime approx. 3h\n",
    "for i, row in df_human_vs_llm_sampled.iterrows():\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"Processing row {i+1}/{len(df_human_vs_llm_sampled)}\")\n",
    "    text = row[\"text\"]\n",
    "    tokens = list(tokenize(text, lowercase=True))\n",
    "    vectors = np.asarray([model[w] for w in tokens if w in model])\n",
    "    \n",
    "    # Skip if not enough vectors\n",
    "    if len(vectors) < 2:\n",
    "        continue\n",
    "    \n",
    "    # For each pool_order, compute CoCo values\n",
    "    for p_order in pool_orders:\n",
    "        lrd = LRDEstimator.LRDEstimator(vectors)\n",
    "        max_lag = int(vectors.shape[0] / 2)\n",
    "        current_lag = 1\n",
    "        while current_lag < max_lag:\n",
    "            coco_value = lrd.calculate_coco(lag=current_lag, pool_order=p_order)\n",
    "            coco_results.append({\n",
    "                \"text\": text,\n",
    "                \"source\": row[\"source\"],\n",
    "                \"pool_order\": p_order,\n",
    "                \"lag\": int(current_lag),\n",
    "                \"coco_value\": coco_value,\n",
    "            })\n",
    "            current_lag = int(np.ceil(current_lag * 1.1))\n",
    "        \n",
    "    \n",
    "    # For each pool_order, fit the curves\n",
    "    for p_order in pool_orders:\n",
    "        # Filter the relevant CoCo results\n",
    "        book_data = [\n",
    "            r for r in coco_results \n",
    "            if (r[\"text\"] == text) and (r[\"pool_order\"] == p_order)\n",
    "        ]\n",
    "        if not book_data:\n",
    "            continue\n",
    "        \n",
    "        lags = np.array([d[\"lag\"] for d in book_data])\n",
    "        c_coco = np.array([d[\"coco_value\"] for d in book_data])\n",
    "        abs_coco = np.abs(c_coco)\n",
    "        \n",
    "        # Fit power law\n",
    "        try:\n",
    "            popt_pl, _ = curve_fit(\n",
    "                utils.power_law,\n",
    "                lags,\n",
    "                abs_coco,\n",
    "                bounds=([-np.inf, -np.inf], [np.inf, 0]),\n",
    "                maxfev=5000\n",
    "            )\n",
    "            gamma_col = f\"gamma_{p_order}\"\n",
    "            df_human_vs_llm_sampled.loc[i, gamma_col] = popt_pl[1]\n",
    "        except RuntimeError:\n",
    "            pass  # If fitting fails, leave as NaN\n",
    "        \n",
    "        # Fit stretched exponential\n",
    "        try:\n",
    "            popt_se, _ = curve_fit(\n",
    "                utils.stretched_exponential,\n",
    "                lags,\n",
    "                abs_coco,\n",
    "                bounds=([0, 0, -np.inf], [np.inf, 1, np.inf]),\n",
    "                maxfev=5000\n",
    "            )\n",
    "            delta_col = f\"delta_{p_order}\"\n",
    "            beta_col = f\"beta_{p_order}\"\n",
    "            df_human_vs_llm_sampled.loc[i, delta_col] = popt_se[0]\n",
    "            df_human_vs_llm_sampled.loc[i, beta_col] = popt_se[1]\n",
    "        except RuntimeError:\n",
    "            pass  # If fitting fails, leave as NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ffbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "df_human_vs_llm_sampled.to_csv(\"results/human_vs_llm_sampled_after.csv\", index=False)\n",
    "coco_results_df = pd.DataFrame(coco_results)\n",
    "coco_results_df.to_csv(\"results/human_vs_llm_coco_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89d46c",
   "metadata": {},
   "source": [
    "### Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f58fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results for further analysis\n",
    "df_human_vs_llm_sampled = pd.read_csv(\"results/human_vs_llm_sampled_after.csv\")\n",
    "\n",
    "# Explore missing values in the fitted parameters\n",
    "selected_columns = df_human_vs_llm_sampled.filter(regex=\"^(gamma|delta|beta)\").columns\n",
    "missing_values = df_human_vs_llm_sampled[selected_columns].isnull().sum()\n",
    "missing_percentage = (missing_values / len(df_human_vs_llm_sampled)) * 100\n",
    "\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    \"Parameter Name\": missing_values.index,\n",
    "    \"Missing Count\": missing_values.values,\n",
    "    \"Missing Percentage\": missing_percentage.values\n",
    "})\n",
    "\n",
    "missing_data_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse sign of gamma\n",
    "df_human_vs_llm_sampled[\"gamma_1\"] = -df_human_vs_llm_sampled[\"gamma_1\"]\n",
    "df_human_vs_llm_sampled[\"gamma_3\"] = -df_human_vs_llm_sampled[\"gamma_3\"]\n",
    "df_human_vs_llm_sampled[\"gamma_9\"] = -df_human_vs_llm_sampled[\"gamma_9\"]\n",
    "df_human_vs_llm_sampled[\"gamma_27\"] = -df_human_vs_llm_sampled[\"gamma_27\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87431d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization to compare parameters across languages\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle('Distribution of Parameters Across Sources', fontsize=16)\n",
    "\n",
    "# Set up the parameters to plot\n",
    "params = [\n",
    "    [\"gamma_1\", \"gamma_3\", \"gamma_9\", \"gamma_27\"],\n",
    "    [\"delta_1\", \"delta_3\", \"delta_9\", \"delta_27\"],\n",
    "    [\"beta_1\", \"beta_3\", \"beta_9\", \"beta_27\"]\n",
    "]\n",
    "\n",
    "# Create a boxplot for each parameter\n",
    "for i, param_row in enumerate(params):\n",
    "    for j, param in enumerate(param_row):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        # Create a temporary dataframe with just what we need\n",
    "        plot_data = []\n",
    "        languages = []\n",
    "        \n",
    "        for lang in df_human_vs_llm_sampled['source'].unique():\n",
    "            data = df_human_vs_llm_sampled.loc[df_human_vs_llm_sampled['source'] == lang, param].dropna()\n",
    "            if len(data) > 0:\n",
    "                plot_data.append(data)\n",
    "                # Clean up language code for display\n",
    "                lang_code = lang.strip(\"[]'\")\n",
    "                languages.append(lang_code)\n",
    "        \n",
    "        # Create the boxplot\n",
    "        bp = ax.boxplot(plot_data, patch_artist=True, showfliers=False)\n",
    "        \n",
    "        # Set colors for each boxplot\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(plot_data)))\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "        \n",
    "        # Set plot title and labels\n",
    "        ax.set_title(param)\n",
    "        ax.set_xticklabels(languages, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Value')\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store the Kruskal-Wallis test results\n",
    "kw_results = pd.DataFrame(\n",
    "    index=['gamma', 'delta', 'beta'],\n",
    "    columns=['1', '3', '9', '27']\n",
    ")\n",
    "\n",
    "# Define parameter groups\n",
    "param_groups = {\n",
    "    'gamma': ['gamma_1', 'gamma_3', 'gamma_9', 'gamma_27'],\n",
    "    'delta': ['delta_1', 'delta_3', 'delta_9', 'delta_27'],\n",
    "    'beta': ['beta_1', 'beta_3', 'beta_9', 'beta_27']\n",
    "}\n",
    "\n",
    "# Perform Kruskal-Wallis test for each parameter\n",
    "for group_name, group_params in param_groups.items():\n",
    "    for param in group_params:\n",
    "        pool_order = param.split('_')[1]  # Extract pool order (1, 3, 9, 27)\n",
    "        \n",
    "        # Create a list of data for each language\n",
    "        groups = []\n",
    "        group_labels = []\n",
    "        \n",
    "        for lang in df_human_vs_llm_sampled['source'].unique():\n",
    "            data = df_human_vs_llm_sampled.loc[df_human_vs_llm_sampled['source'] == lang, param].dropna()\n",
    "            if len(data) > 0:\n",
    "                groups.append(data)\n",
    "                group_labels.append(lang)\n",
    "        \n",
    "        # Perform Kruskal-Wallis test\n",
    "        if len(groups) > 1:  # Need at least 2 groups for the test\n",
    "            stat, p_value = sps.kruskal(*groups)\n",
    "            kw_results.loc[group_name, pool_order] = p_value\n",
    "        else:\n",
    "            kw_results.loc[group_name, pool_order] = float('nan')\n",
    "\n",
    "# Format p-values with scientific notation for small values\n",
    "kw_results_formatted = kw_results.applymap(lambda x: f\"{x:.2e}\" if pd.notnull(x) else \"NaN\")\n",
    "\n",
    "# Display the results\n",
    "kw_results_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660ff418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform post-hoc Dunn test with Bonferroni correction for each parameter\n",
    "# We'll create a function to run the test and visualize the results\n",
    "\n",
    "def perform_dunn_test_and_visualize(parameter_groups, df_human_vs_llm_sampled):\n",
    "    \"\"\"\n",
    "    Perform Dunn's post-hoc test with Bonferroni correction for each parameter group\n",
    "    and visualize the results using heatmaps.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "    fig.suptitle('Dunn Test Results (p-values with Bonferroni correction)', fontsize=18)\n",
    "    \n",
    "    # Define symbols for parameter groups for better visualization\n",
    "    param_symbols = {\n",
    "        'gamma': '$\\gamma$',\n",
    "        'delta': '$\\delta$',\n",
    "        'beta': '$\\\\beta$'\n",
    "    }\n",
    "    \n",
    "    # Loop through parameter groups\n",
    "    for row, (group_name, params) in enumerate(parameter_groups.items()):\n",
    "        param_symbol = param_symbols.get(group_name, group_name)\n",
    "        \n",
    "        for col, param in enumerate(params):\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Extract language and data for Dunn test\n",
    "            groups = []\n",
    "            lang_codes = []\n",
    "            \n",
    "            for lang in df_human_vs_llm_sampled['source'].unique():\n",
    "                data = df_human_vs_llm_sampled.loc[df_human_vs_llm_sampled['source'] == lang, param].dropna()\n",
    "                if len(data) > 0:\n",
    "                    groups.append(data.values)\n",
    "                    lang_code = lang.strip(\"[]'\")\n",
    "                    lang_codes.append(lang_code)\n",
    "            \n",
    "            # Perform Dunn test\n",
    "            if len(groups) > 2:  # Need at least 3 groups for post-hoc\n",
    "                try:\n",
    "                    # Perform Dunn test with Bonferroni correction\n",
    "                    posthoc_results = sp.posthoc_dunn(groups, p_adjust='bonferroni')\n",
    "                    \n",
    "                    # Create a heatmap for the p-values\n",
    "                    sns.heatmap(posthoc_results, annot=True, cmap='coolwarm_r', \n",
    "                               vmin=0, vmax=0.05, ax=ax, xticklabels=lang_codes, \n",
    "                               yticklabels=lang_codes, cbar=False, fmt='.3f')\n",
    "                    \n",
    "                    # Color p-values < 0.05 differently for emphasis\n",
    "                    for i in range(posthoc_results.shape[0]):\n",
    "                        for j in range(posthoc_results.shape[1]):\n",
    "                            if posthoc_results.iloc[i, j] < 0.05:\n",
    "                                ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=False, edgecolor='black', lw=1.5))\n",
    "                    \n",
    "                    # Set plot title and labels\n",
    "                    ax.set_title(f\"{param_symbol}_{param.split('_')[1]}\")\n",
    "                except Exception as e:\n",
    "                    ax.text(0.5, 0.5, f\"Error: {str(e)}\", horizontalalignment='center',\n",
    "                           verticalalignment='center', transform=ax.transAxes)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, \"Insufficient data for\\npost-hoc test\", horizontalalignment='center',\n",
    "                       verticalalignment='center', transform=ax.transAxes)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Run the post-hoc tests and visualize\n",
    "perform_dunn_test_and_visualize(param_groups, df_human_vs_llm_sampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
