{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as sps\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d907faa",
   "metadata": {},
   "source": [
    "## Reusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842dcf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boxplot(df, col_value, col_category, title):\n",
    "    plot_data = []\n",
    "    groups = []\n",
    "\n",
    "    for group in df[col_category].unique():\n",
    "        group_data = df[df[col_category] == group][col_value].dropna()\n",
    "        plot_data.append(group_data)\n",
    "        groups.append(group)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    bp = ax.boxplot(plot_data, patch_artist=True, showfliers=False)\n",
    "\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(plot_data)))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color) \n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticklabels(groups, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Value')\n",
    "        \n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99881293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(df, col_value, col_category, title):\n",
    "    plot_data = []\n",
    "    groups = []\n",
    "                \n",
    "    for group in df[col_category].unique():\n",
    "        group_data = df[df[col_category] == group][col_value].dropna()\n",
    "        plot_data.append(group_data)\n",
    "        groups.append(group)\n",
    "                \n",
    "    # Perform Dunn test\n",
    "    posthoc_results = sp.posthoc_dunn(plot_data, p_adjust='bonferroni')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(\n",
    "        posthoc_results,\n",
    "        annot=True,\n",
    "        cmap='coolwarm_r',\n",
    "        vmin=0,\n",
    "        vmax=0.05,\n",
    "        ax=ax,\n",
    "        xticklabels=groups,\n",
    "        yticklabels=groups,\n",
    "        cbar=False,\n",
    "        fmt='.2f',\n",
    "        annot_kws={\"fontsize\":7}\n",
    "    )\n",
    "\n",
    "    # Color p-values < 0.05 differently for emphasis\n",
    "    for i in range(posthoc_results.shape[0]):\n",
    "        for j in range(posthoc_results.shape[1]):\n",
    "            if posthoc_results.iloc[i, j] < 0.05:\n",
    "                ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=False, edgecolor='black', lw=1.5))\n",
    "\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mean_std_table(df, param_name, category_col, decimal_places=3):\n",
    "    \"\"\"\n",
    "    Creates a table with mean ± std values for a parameter across different pool orders.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with the data\n",
    "        param_name: Base name of the parameter (e.g., 'gamma', 'delta', 'beta')\n",
    "        category_col: Column name for categories (e.g., 'language', 'source')\n",
    "        decimal_places: Number of decimal places for formatting (ignored now, kept for backward compatibility)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with mean ± std values\n",
    "    \"\"\"\n",
    "    # Pool orders we're interested in\n",
    "    pool_orders = [1, 3, 9, 27]\n",
    "    \n",
    "    # Create an empty DataFrame for results\n",
    "    categories = sorted(df[category_col].unique())\n",
    "    result_df = pd.DataFrame(index=categories, columns=pool_orders)\n",
    "    \n",
    "    # Calculate mean and std for each category and pool order\n",
    "    for category in categories:\n",
    "        for order in pool_orders:\n",
    "            param_col = f\"{param_name}_{order}\"\n",
    "            category_data = df[df[category_col] == category][param_col].dropna()\n",
    "            \n",
    "            if len(category_data) > 0:\n",
    "                mean_val = category_data.mean()\n",
    "                std_val = category_data.std()\n",
    "                \n",
    "                # Format with 2 significant digits for std and matching precision for mean\n",
    "                if pd.isna(std_val) or std_val == 0:\n",
    "                    result_df.loc[category, order] = f\"{mean_val:.2f} ± 0.00\"\n",
    "                else:\n",
    "                    # Determine the order of magnitude of std\n",
    "                    magnitude = int(np.floor(np.log10(abs(std_val))))\n",
    "                    \n",
    "                    # Calculate decimal places needed for 2 significant digits in std\n",
    "                    significant_places = max(0, 2 - magnitude - 1)\n",
    "                    \n",
    "                    # Format the values with the appropriate precision\n",
    "                    format_str = f\"{{:.{significant_places}f}} ± {{:.{significant_places}f}}\"\n",
    "                    result_df.loc[category, order] = format_str.format(\n",
    "                        round(mean_val, significant_places), \n",
    "                        round(std_val, significant_places)\n",
    "                    )\n",
    "            else:\n",
    "                result_df.loc[category, order] = \"N/A\"\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def export_table_to_latex(df, caption, label):\n",
    "    \"\"\"\n",
    "    Exports a DataFrame to LaTeX format with proper styling.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to export\n",
    "        caption: Caption for the table\n",
    "        label: Label for the table\n",
    "        \n",
    "    Returns:\n",
    "        LaTeX code as a string\n",
    "    \"\"\"\n",
    "    # Start of LaTeX table\n",
    "    latex_code = \"\\\\begin{table}[p]\\n\\\\centering\\n\"\n",
    "    \n",
    "    # Get column headers\n",
    "    columns = df.columns\n",
    "    num_columns = len(columns)\n",
    "    \n",
    "    # Create tabular environment\n",
    "    tabular_spec = \"l\" + \"r\" * num_columns\n",
    "    latex_code += f\"\\\\begin{{tabular}}{{{tabular_spec}}}\\n\"\n",
    "    \n",
    "    # Top rule\n",
    "    latex_code += \"  \\\\toprule\\n\"\n",
    "    \n",
    "    # Header row\n",
    "    header_row = \"  & \" + \" & \".join([f\"\\\\multicolumn{{1}}{{c}}{{{col}}}\" for col in columns]) + \" \\\\\\\\\\n\"\n",
    "    latex_code += header_row\n",
    "    \n",
    "    # Middle rule\n",
    "    latex_code += \"  \\\\midrule\\n\"\n",
    "    \n",
    "    # Data rows\n",
    "    for idx, row in df.iterrows():\n",
    "        row_values = [str(idx)] + [str(val) for val in row.values]\n",
    "        latex_code += \"  \" + \" & \".join(row_values) + \" \\\\\\\\\\n\"\n",
    "    \n",
    "    # Bottom rule\n",
    "    latex_code += \"  \\\\bottomrule\\n\"\n",
    "    \n",
    "    # End of tabular environment\n",
    "    latex_code += \"\\\\end{tabular}\\n\"\n",
    "    \n",
    "    # Caption and label\n",
    "    latex_code += f\"\\\\caption{{{caption}}}\\n\"\n",
    "    latex_code += f\"\\\\label{{{label}}}\\n\"\n",
    "    \n",
    "    # End of table environment\n",
    "    latex_code += \"\\\\end{table}\"\n",
    "    \n",
    "    return latex_code\n",
    "\n",
    "def collect_tables_to_file(tables, filename, title):\n",
    "    \"\"\"\n",
    "    Collects multiple LaTeX tables and saves them to a single file.\n",
    "    \n",
    "    Args:\n",
    "        tables: List of LaTeX table code strings\n",
    "        filename: Output filename to save the combined tables\n",
    "        title: Title for the collection of tables\n",
    "    \"\"\"\n",
    "    # Start the LaTeX document\n",
    "    latex_content = \"\\\\documentclass{article}\\n\\\\usepackage{booktabs}\\n\\\\usepackage{amsmath}\\n\"\n",
    "    latex_content += \"\\\\title{\" + title + \"}\\n\\\\begin{document}\\n\\\\maketitle\\n\\n\"\n",
    "    \n",
    "    # Add all tables\n",
    "    for table in tables:\n",
    "        latex_content += table + \"\\n\\n\"\n",
    "    \n",
    "    # End the LaTeX document\n",
    "    latex_content += \"\\\\end{document}\"\n",
    "    \n",
    "    # Write to file\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(latex_content)\n",
    "        \n",
    "    print(f\"All tables saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8af8bd",
   "metadata": {},
   "source": [
    "## Standardized Project Gutenberg Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d669a28",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results for further analysis\n",
    "spgc_metadata_sampled = pd.read_csv(\"results/spgc_metadata_sampled_after.csv\")\n",
    "\n",
    "# Reverse sign of gamma\n",
    "spgc_metadata_sampled[\"gamma_0\"] = -spgc_metadata_sampled[\"gamma_0\"]\n",
    "spgc_metadata_sampled[\"gamma_3\"] = -spgc_metadata_sampled[\"gamma_3\"]\n",
    "spgc_metadata_sampled[\"gamma_9\"] = -spgc_metadata_sampled[\"gamma_9\"]\n",
    "spgc_metadata_sampled[\"gamma_27\"] = -spgc_metadata_sampled[\"gamma_27\"]\n",
    "\n",
    "# Rename columns _0 to _1\n",
    "spgc_metadata_sampled.rename(columns=lambda x: x.replace(\"_0\", \"_1\"), inplace=True)\n",
    "\n",
    "# Skip Chinese language, because embeddings from NLTK are not well-suited for SPGC corpus\n",
    "spgc_metadata_sampled = spgc_metadata_sampled[spgc_metadata_sampled[\"language\"] != \"['zh']\"]\n",
    "\n",
    "# Strip brackets from language codes\n",
    "spgc_metadata_sampled[\"language\"] = spgc_metadata_sampled[\"language\"].str.replace(r\"[\\[\\]']\", \"\", regex=True)\n",
    "\n",
    "# Explore missing values in the fitted parameters\n",
    "selected_columns = spgc_metadata_sampled.filter(regex=\"^(gamma|delta|beta)\").columns\n",
    "missing_values = spgc_metadata_sampled[selected_columns].isnull().sum()\n",
    "missing_percentage = (missing_values / len(spgc_metadata_sampled)) * 100\n",
    "\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    \"Parameter Name\": missing_values.index,\n",
    "    \"Missing Count\": missing_values.values,\n",
    "    \"Missing Percentage\": missing_percentage.values\n",
    "})\n",
    "\n",
    "missing_data_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb6daf",
   "metadata": {},
   "source": [
    "### Failure ratio and error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f113163",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_orders = [1, 3, 9, 27]\n",
    "for order in pool_orders:\n",
    "    fig, ax = plt.subplots()\n",
    "    create_boxplot(\n",
    "        spgc_metadata_sampled,\n",
    "        col_value=f\"error_pl_{order}\",\n",
    "        col_category=\"language\",\n",
    "        title=f\"SSLR for power law (order {order})\"\n",
    "    )\n",
    "    pdf_filename = f\"figures/spgc_arsr_pl_{order}.pdf\"\n",
    "    plt.savefig(pdf_filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    create_boxplot(\n",
    "        spgc_metadata_sampled,\n",
    "        col_value=f\"error_se_{order}\",\n",
    "        col_category=\"language\",\n",
    "        title=f\"SSLR for stretched exponential (order {order})\"\n",
    "    )\n",
    "    pdf_filename = f\"figures/spgc_arsr_se_{order}.pdf\"\n",
    "    plt.savefig(pdf_filename)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74606eb4",
   "metadata": {},
   "source": [
    "### Distribution of fitted parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca50d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter base names for tables\n",
    "param_bases = [\"gamma\", \"alpha_pl\", \"delta\", \"beta\", \"alpha_se\"]\n",
    "\n",
    "# Create a list to collect all tables\n",
    "spgc_tables = []\n",
    "\n",
    "# Generate tables for each parameter type\n",
    "for param_base in param_bases:\n",
    "    # Create and save the table\n",
    "    table_df = create_mean_std_table(\n",
    "        spgc_metadata_sampled, \n",
    "        param_base, \n",
    "        \"language\", \n",
    "        decimal_places=2\n",
    "    )\n",
    "    \n",
    "    # Create a display name for the parameter\n",
    "    if param_base == \"gamma\":\n",
    "        param_display = \"\\\\gamma\"\n",
    "        param_name = \"gamma\"\n",
    "    elif param_base == \"delta\":\n",
    "        param_display = \"\\\\delta\"\n",
    "        param_name = \"delta\"\n",
    "    elif param_base == \"beta\":\n",
    "        param_display = \"\\\\beta\"\n",
    "        param_name = \"beta\"\n",
    "    elif param_base == \"alpha_pl\":\n",
    "        param_display = \"c\"\n",
    "        param_name = \"c\"\n",
    "    elif param_base == \"alpha_se\":\n",
    "        param_display = \"b\"\n",
    "        param_name = \"b\"\n",
    "    \n",
    "    caption = f\"Mean and standard deviation of ${param_display}$ parameter for different languages\"\n",
    "    label = f\"tab:spgc_{param_name}_mean_std\"\n",
    "    \n",
    "    # Export to LaTeX and collect\n",
    "    latex_code = export_table_to_latex(table_df, caption, label)\n",
    "    spgc_tables.append(latex_code)\n",
    "    \n",
    "    # Print the table\n",
    "    print(f\"Table for {param_base}:\")\n",
    "    print(table_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Save all tables to a single file\n",
    "collect_tables_to_file(spgc_tables, \"figures/spgc_all_tables.tex\", \"SPGC Parameter Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = [\n",
    "    \"gamma_1\", \"gamma_3\", \"gamma_9\", \"gamma_27\",\n",
    "    \"alpha_pl_1\", \"alpha_pl_3\", \"alpha_pl_9\", \"alpha_pl_27\",\n",
    "    \"delta_1\", \"delta_3\", \"delta_9\", \"delta_27\",\n",
    "    \"beta_1\", \"beta_3\", \"beta_9\", \"beta_27\",\n",
    "    \"alpha_se_1\", \"alpha_se_3\", \"alpha_se_9\", \"alpha_se_27\",\n",
    "]\n",
    "\n",
    "for i, param in enumerate(all_params, 1):\n",
    "    # Titles with greek letters\n",
    "    if param.startswith(\"gamma\"):\n",
    "        title = r\"Fitted $\\gamma$ (order {}) for all languages\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"delta\"):\n",
    "        title = r\"Fitted $\\delta$ (order {}) for all languages\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"beta\"):\n",
    "        title = r\"Fitted $\\beta$ (order {}) for all languages\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"alpha_pl\"):\n",
    "        title = r\"Fitted $c$ (order {}) for all languages\".format(param.split(\"_\")[2])\n",
    "    elif param.startswith(\"alpha_se\"):\n",
    "        title = r\"Fitted $b$ (order {}) for all languages\".format(param.split(\"_\")[2])\n",
    "    else:\n",
    "        title = f\"Fitted {param} for all languages\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    create_boxplot(\n",
    "        df = spgc_metadata_sampled,\n",
    "        col_value = param,\n",
    "        col_category = \"language\",\n",
    "        title = title\n",
    "    )\n",
    "\n",
    "    if param.startswith(\"alpha_pl\"):\n",
    "        param_name = \"c\" + param.split(\"_\")[2]\n",
    "    elif param.startswith(\"alpha_se\"):\n",
    "        param_name = \"b\" + param.split(\"_\")[2]\n",
    "    else:\n",
    "        param_name = param\n",
    "\n",
    "    pdf_filename = f\"figures/spgc_{param_name}.pdf\"\n",
    "    plt.savefig(pdf_filename)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336bfa37",
   "metadata": {},
   "source": [
    "### Analysis of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de750bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store the Kruskal-Wallis test results\n",
    "kw_results = pd.DataFrame(\n",
    "    index=['gamma', 'delta', 'beta'],\n",
    "    columns=['1', '3', '9', '27']\n",
    ")\n",
    "\n",
    "# Define parameter groups\n",
    "param_groups = {\n",
    "    'gamma': ['gamma_1', 'gamma_3', 'gamma_9', 'gamma_27'],\n",
    "    'delta': ['delta_1', 'delta_3', 'delta_9', 'delta_27'],\n",
    "    'beta': ['beta_1', 'beta_3', 'beta_9', 'beta_27']\n",
    "}\n",
    "\n",
    "# Perform Kruskal-Wallis test for each parameter\n",
    "for group_name, group_params in param_groups.items():\n",
    "    for param in group_params:\n",
    "        pool_order = param.split('_')[1]  # Extract pool order (1, 3, 9, 27)\n",
    "        \n",
    "        # Create a list of data for each language\n",
    "        groups = []\n",
    "        group_labels = []\n",
    "        \n",
    "        for lang in spgc_metadata_sampled['language'].unique():\n",
    "            data = spgc_metadata_sampled.loc[spgc_metadata_sampled['language'] == lang, param].dropna()\n",
    "            if len(data) > 0:\n",
    "                groups.append(data)\n",
    "                group_labels.append(lang)\n",
    "        \n",
    "        # Perform Kruskal-Wallis test\n",
    "        if len(groups) > 1:  # Need at least 2 groups for the test\n",
    "            stat, p_value = sps.kruskal(*groups)\n",
    "            kw_results.loc[group_name, pool_order] = p_value\n",
    "        else:\n",
    "            kw_results.loc[group_name, pool_order] = float('nan')\n",
    "\n",
    "# Format p-values with scientific notation for small values\n",
    "kw_results_formatted = kw_results.applymap(lambda x: f\"{x:.2e}\" if pd.notnull(x) else \"NaN\")\n",
    "\n",
    "# Display the results\n",
    "kw_results_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb23d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(all_params, 1):\n",
    "    # Titles with greek letters\n",
    "    if param.startswith(\"gamma\"):\n",
    "        title = r\"p-value of Dunn test for $\\gamma$ (order {})\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"delta\"):\n",
    "        title = r\"Fp-value of Dunn test for $\\delta$ (order {})\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"beta\"):\n",
    "        title = r\"p-value of Dunn test for $\\beta$ (order {})\".format(param.split(\"_\")[1])\n",
    "    else:\n",
    "        title = f\"p-value of Dunn test for {param}\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    create_heatmap(\n",
    "        df = spgc_metadata_sampled,\n",
    "        col_value = param,\n",
    "        col_category = \"language\",\n",
    "        title = title\n",
    "    )\n",
    "    pdf_filename = f\"figures/spgc_heatmap-{i}.pdf\"\n",
    "    plt.savefig(pdf_filename)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4856dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate mean and std with consistent formatting\n",
    "def format_mean_std(data, is_percentage=False):\n",
    "    \"\"\"Format mean ± std with 2 significant digits for std, skipping infinity values\"\"\"\n",
    "    # Filter out infinity values\n",
    "    filtered_data = data[~np.isinf(data)]\n",
    "    \n",
    "    # Check if we have any data left after filtering\n",
    "    if len(filtered_data) == 0:\n",
    "        return \"N/A\"  # No finite values available\n",
    "    \n",
    "    mean_val = filtered_data.mean()\n",
    "    std_val = filtered_data.std()\n",
    "    \n",
    "    if pd.isna(std_val) or std_val == 0:\n",
    "        return f\"{mean_val:.2f} ± 0.00\"\n",
    "    \n",
    "    # Determine order of magnitude of std\n",
    "    magnitude = int(np.floor(np.log10(abs(std_val))))\n",
    "    \n",
    "    # Calculate decimal places needed for 2 significant digits in std\n",
    "    significant_places = max(0, 2 - magnitude - 1)\n",
    "    \n",
    "    # Apply percentage multiplier if needed\n",
    "    if is_percentage:\n",
    "        mean_val *= 100\n",
    "        std_val *= 100\n",
    "    \n",
    "    # Format with proper precision\n",
    "    format_str = f\"{{:.{significant_places}f}} ± {{:.{significant_places}f}}\"\n",
    "    return format_str.format(round(mean_val, significant_places), round(std_val, significant_places))\n",
    "\n",
    "# Create failure rates DataFrames with proper formatting\n",
    "pooling_orders = ['1', '3', '9', '27']\n",
    "pl_failure_rates = []\n",
    "pl_avg_errors = []\n",
    "se_failure_rates = []\n",
    "se_avg_errors = []\n",
    "\n",
    "for order in [1, 3, 9, 27]:\n",
    "    # PL failure rates (as simple percentages)\n",
    "    pl_null_data = spgc_metadata_sampled[f\"gamma_{order}\"].isnull()\n",
    "    pl_null_percentage = 100 * pl_null_data.mean()  # mean of boolean series = proportion of True values\n",
    "    pl_failure_rates.append(f\"{pl_null_percentage:.2f}%\")\n",
    "    \n",
    "    # PL avg errors\n",
    "    pl_error_data = spgc_metadata_sampled[f\"error_pl_{order}\"].dropna()\n",
    "    pl_avg_errors.append(format_mean_std(pl_error_data))\n",
    "    \n",
    "    # SE failure rates (as simple percentages)\n",
    "    se_null_data = spgc_metadata_sampled[f\"delta_{order}\"].isnull()\n",
    "    se_null_percentage = 100 * se_null_data.mean()\n",
    "    se_failure_rates.append(f\"{se_null_percentage:.2f}%\")\n",
    "    \n",
    "    # SE avg errors\n",
    "    se_error_data = spgc_metadata_sampled[f\"error_se_{order}\"].dropna()\n",
    "    se_avg_errors.append(format_mean_std(se_error_data))\n",
    "\n",
    "# Create the table\n",
    "failure_rates_df = pd.DataFrame({\n",
    "    'Pooling order': pooling_orders,\n",
    "    'PL failure rate': pl_failure_rates,\n",
    "    'PL avg. SSLR': pl_avg_errors,\n",
    "    'SE failure rate': se_failure_rates,\n",
    "    'SE avg. SSLR': se_avg_errors\n",
    "})\n",
    "\n",
    "# Export to LaTeX with consistent formatting\n",
    "failure_rates_latex = export_table_to_latex(\n",
    "    failure_rates_df, \n",
    "    caption=\"Failure rates and average error metrics for SPGC corpus\", \n",
    "    label=\"tab:spgc_failure_rates\"\n",
    ")\n",
    "\n",
    "print(failure_rates_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ee987",
   "metadata": {},
   "source": [
    "## Human vs LLM Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f3ff1",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac16147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results for further analysis\n",
    "df_human_vs_llm_sampled = pd.read_csv(\"results/human_vs_llm_sampled_after.csv\")\n",
    "\n",
    "# Reverse sign of gamma\n",
    "df_human_vs_llm_sampled[\"gamma_1\"] = -df_human_vs_llm_sampled[\"gamma_1\"]\n",
    "df_human_vs_llm_sampled[\"gamma_3\"] = -df_human_vs_llm_sampled[\"gamma_3\"]\n",
    "df_human_vs_llm_sampled[\"gamma_9\"] = -df_human_vs_llm_sampled[\"gamma_9\"]\n",
    "df_human_vs_llm_sampled[\"gamma_27\"] = -df_human_vs_llm_sampled[\"gamma_27\"]\n",
    "\n",
    "# Explore missing values in the fitted parameters\n",
    "selected_columns = df_human_vs_llm_sampled.filter(regex=\"^(gamma|delta|beta)\").columns\n",
    "missing_values = df_human_vs_llm_sampled[selected_columns].isnull().sum()\n",
    "missing_percentage = (missing_values / len(df_human_vs_llm_sampled)) * 100\n",
    "\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    \"Parameter Name\": missing_values.index,\n",
    "    \"Missing Count\": missing_values.values,\n",
    "    \"Missing Percentage\": missing_percentage.values\n",
    "})\n",
    "\n",
    "missing_data_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7cb706",
   "metadata": {},
   "source": [
    "### Failure ratio and error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c975f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_orders = [1, 3, 9, 27]\n",
    "for order in pool_orders:\n",
    "    fig, ax = plt.subplots()\n",
    "    create_boxplot(\n",
    "        df_human_vs_llm_sampled,\n",
    "        col_value=f\"error_pl_{order}\",\n",
    "        col_category=\"source\",\n",
    "        title=f\"SSLR for power law (order {order})\"\n",
    "    )\n",
    "    pdf_filename = f\"figures/hllm_arsr_pl_{order}.pdf\"\n",
    "    plt.savefig(pdf_filename)\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    create_boxplot(\n",
    "        df_human_vs_llm_sampled,\n",
    "        col_value=f\"error_se_{order}\",\n",
    "        col_category=\"source\",\n",
    "        title=f\"SSLR for stretched exponential (order {order})\"\n",
    "    )\n",
    "    pdf_filename = f\"figures/hllm_arsr_se_{order}.pdf\"\n",
    "    plt.savefig(pdf_filename)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ee89f",
   "metadata": {},
   "source": [
    "### Distribution of fitted parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a53955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter base names for tables\n",
    "param_bases = [\"gamma\", \"alpha_pl\", \"delta\", \"beta\", \"alpha_se\"]\n",
    "\n",
    "# Create a list to collect all tables\n",
    "hllm_tables = []\n",
    "\n",
    "# Generate tables for each parameter type\n",
    "for param_base in param_bases:\n",
    "    # Create and save the table\n",
    "    table_df = create_mean_std_table(\n",
    "        df_human_vs_llm_sampled, \n",
    "        param_base, \n",
    "        \"source\", \n",
    "        decimal_places=4\n",
    "    )\n",
    "    \n",
    "    # Create a display name for the parameter\n",
    "    if param_base == \"gamma\":\n",
    "        param_display = \"\\\\gamma\"\n",
    "        param_name = \"gamma\"\n",
    "    elif param_base == \"delta\":\n",
    "        param_display = \"\\\\delta\"\n",
    "        param_name = \"delta\"\n",
    "    elif param_base == \"beta\":\n",
    "        param_display = \"\\\\beta\"\n",
    "        param_name = \"beta\"\n",
    "    elif param_base == \"alpha_pl\":\n",
    "        param_display = \"c\"\n",
    "        param_name = \"c\"\n",
    "    elif param_base == \"alpha_se\":\n",
    "        param_display = \"b\"\n",
    "        param_name = \"b\"\n",
    "    \n",
    "    caption = f\"Mean and standard deviation of ${param_display}$ parameter for different sources\"\n",
    "    label = f\"tab:hllm_{param_name}_mean_std\"\n",
    "    \n",
    "    # Export to LaTeX and collect\n",
    "    latex_code = export_table_to_latex(table_df, caption, label)\n",
    "    hllm_tables.append(latex_code)\n",
    "    \n",
    "    # Print the table\n",
    "    print(f\"Table for {param_base}:\")\n",
    "    print(table_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Save all tables to a single file\n",
    "collect_tables_to_file(hllm_tables, \"figures/hllm_all_tables.tex\", \"Human vs LLM Parameter Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(all_params, 1):\n",
    "    # Titles with greek letters\n",
    "    if param.startswith(\"gamma\"):\n",
    "        title = r\"Fitted $\\gamma$ (order {}) for all sources\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"delta\"):\n",
    "        title = r\"Fitted $\\delta$ (order {}) for all sources\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"beta\"):\n",
    "        title = r\"Fitted $\\beta$ (order {}) for all sources\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"alpha_pl\"):\n",
    "        title = r\"Fitted $c$ (order {}) for all languages\".format(param.split(\"_\")[2])\n",
    "    elif param.startswith(\"alpha_se\"):\n",
    "        title = r\"Fitted $b$ (order {}) for all languages\".format(param.split(\"_\")[2])\n",
    "    else:\n",
    "        title = f\"Fitted {param} for all sources\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    create_boxplot(\n",
    "        df = df_human_vs_llm_sampled,\n",
    "        col_value = param,\n",
    "        col_category = \"source\",\n",
    "        title = title\n",
    "    )\n",
    "\n",
    "    if param.startswith(\"alpha_pl\"):\n",
    "        param_name = \"c\" + param.split(\"_\")[2]\n",
    "    elif param.startswith(\"alpha_se\"):\n",
    "        param_name = \"b\" + param.split(\"_\")[2]\n",
    "    else:\n",
    "        param_name = param\n",
    "\n",
    "    pdf_filename = f\"figures/hllm_{param_name}.pdf\"\n",
    "    plt.savefig(pdf_filename)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404e2ff",
   "metadata": {},
   "source": [
    "### Analysis of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc399ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store the Kruskal-Wallis test results\n",
    "kw_results = pd.DataFrame(\n",
    "    index=['gamma', 'delta', 'beta'],\n",
    "    columns=['1', '3', '9', '27']\n",
    ")\n",
    "\n",
    "# Define parameter groups\n",
    "param_groups = {\n",
    "    'gamma': ['gamma_1', 'gamma_3', 'gamma_9', 'gamma_27'],\n",
    "    'delta': ['delta_1', 'delta_3', 'delta_9', 'delta_27'],\n",
    "    'beta': ['beta_1', 'beta_3', 'beta_9', 'beta_27']\n",
    "}\n",
    "\n",
    "# Perform Kruskal-Wallis test for each parameter\n",
    "for group_name, group_params in param_groups.items():\n",
    "    for param in group_params:\n",
    "        pool_order = param.split('_')[1]  # Extract pool order (1, 3, 9, 27)\n",
    "        \n",
    "        # Create a list of data for each language\n",
    "        groups = []\n",
    "        group_labels = []\n",
    "        \n",
    "        for lang in df_human_vs_llm_sampled['source'].unique():\n",
    "            data = df_human_vs_llm_sampled.loc[df_human_vs_llm_sampled['source'] == lang, param].dropna()\n",
    "            if len(data) > 0:\n",
    "                groups.append(data)\n",
    "                group_labels.append(lang)\n",
    "        \n",
    "        # Perform Kruskal-Wallis test\n",
    "        if len(groups) > 1:  # Need at least 2 groups for the test\n",
    "            stat, p_value = sps.kruskal(*groups)\n",
    "            kw_results.loc[group_name, pool_order] = p_value\n",
    "        else:\n",
    "            kw_results.loc[group_name, pool_order] = float('nan')\n",
    "\n",
    "# Format p-values with scientific notation for small values\n",
    "kw_results_formatted = kw_results.applymap(lambda x: f\"{x:.2e}\" if pd.notnull(x) else \"NaN\")\n",
    "\n",
    "# Display the results\n",
    "kw_results_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(all_params, 1):\n",
    "    # Titles with greek letters\n",
    "    if param.startswith(\"gamma\"):\n",
    "        title = r\"p-value of Dunn test for $\\gamma$ (order {})\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"delta\"):\n",
    "        title = r\"Fp-value of Dunn test for $\\delta$ (order {})\".format(param.split(\"_\")[1])\n",
    "    elif param.startswith(\"beta\"):\n",
    "        title = r\"p-value of Dunn test for $\\beta$ (order {})\".format(param.split(\"_\")[1])\n",
    "    else:\n",
    "        title = f\"p-value of Dunn test for {param}\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    create_heatmap(\n",
    "        df = df_human_vs_llm_sampled,\n",
    "        col_value = param,\n",
    "        col_category = \"source\",\n",
    "        title = title\n",
    "    )\n",
    "    pdf_filename = f\"figures/hllm_heatmap-{i}.pdf\"\n",
    "    plt.savefig(pdf_filename)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cd8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create failure rates DataFrames with proper formatting for human vs LLM\n",
    "pooling_orders = ['1', '3', '9', '27']\n",
    "pl_failure_rates = []\n",
    "pl_avg_errors = []\n",
    "se_failure_rates = []\n",
    "se_avg_errors = []\n",
    "\n",
    "for order in [1, 3, 9, 27]:\n",
    "    # PL failure rates (as simple percentages)\n",
    "    pl_null_data = df_human_vs_llm_sampled[f\"gamma_{order}\"].isnull()\n",
    "    pl_null_percentage = 100 * pl_null_data.mean()\n",
    "    pl_failure_rates.append(f\"{pl_null_percentage:.2f}%\")\n",
    "    \n",
    "    # PL avg errors\n",
    "    pl_error_data = df_human_vs_llm_sampled[f\"error_pl_{order}\"].dropna()\n",
    "    pl_avg_errors.append(format_mean_std(pl_error_data))\n",
    "    \n",
    "    # SE failure rates (as simple percentages)\n",
    "    se_null_data = df_human_vs_llm_sampled[f\"delta_{order}\"].isnull()\n",
    "    se_null_percentage = 100 * se_null_data.mean()\n",
    "    se_failure_rates.append(f\"{se_null_percentage:.2f}%\")\n",
    "    \n",
    "    # SE avg errors\n",
    "    se_error_data = df_human_vs_llm_sampled[f\"error_se_{order}\"].dropna()\n",
    "    se_avg_errors.append(format_mean_std(se_error_data))\n",
    "\n",
    "# Create the table\n",
    "failure_rates_df = pd.DataFrame({\n",
    "    'Pooling order': pooling_orders,\n",
    "    'PL failure rate': pl_failure_rates,\n",
    "    'PL avg. SSLR': pl_avg_errors,\n",
    "    'SE failure rate': se_failure_rates,\n",
    "    'SE avg. SSLR': se_avg_errors\n",
    "})\n",
    "\n",
    "# Export to LaTeX with consistent formatting\n",
    "failure_rates_latex = export_table_to_latex(\n",
    "    failure_rates_df, \n",
    "    caption=\"Failure rates and average error metrics for Human vs LLM corpus\", \n",
    "    label=\"tab:hllm_failure_rates\"\n",
    ")\n",
    "\n",
    "print(failure_rates_latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
