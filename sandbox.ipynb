{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External modules\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import os.path\n",
    "\n",
    "# Internal modules\n",
    "from utils import power_law, stretched_exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained *word2vec* embedding space. Below we load space for English dictionary of about ~3 mln words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_embed(file_path: str) -> np.ndarray:\n",
    "    if os.path.exists(file_path):\n",
    "        text_file = open(file_path, mode = \"r\", encoding = \"UTF-8\")\n",
    "    else:\n",
    "        raise ValueError(\"File does not exists or incorrect path provided.\")\n",
    "    \n",
    "    tokens = text_file.read().split(\"\\n\")\n",
    "    vectors = np.asarray([word2vec[w] for w in tokens if w in word2vec])\n",
    "    print(f\"Coverage: {len(vectors) / len(tokens):.4f}.\")\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRDEstimator:\n",
    "    def __init__(self, vectors: np.ndarray):\n",
    "        self.vectors = vectors\n",
    "        self.N, self.d = vectors.shape\n",
    "\n",
    "        # Precompute norms and normalize vectors\n",
    "        self.norms = np.linalg.norm(self.vectors, axis=1, keepdims=True) # shape (N, 1)\n",
    "        self.X = self.vectors / self.norms # shape (N, d)\n",
    "\n",
    "        # Calculate partial sums\n",
    "        self.S = np.zeros((self.N+1, self.d))\n",
    "        self.S[1:] = np.cumsum(self.X, axis=0)\n",
    "\n",
    "        self.polarities = None\n",
    "    \n",
    "    def calculate_polarities(self, standardize: bool = False):\n",
    "        if standardize:\n",
    "            means = np.mean(self.vectors, axis=1, keepdims=True)\n",
    "            stds = np.std(self.vectors, axis=1, keepdims=True)\n",
    "        \n",
    "            stds[stds == 0] = 1 # Avoid division by zero - if variance is zero then calculate V - EV\n",
    "            vectors_standardized = (self.vectors - means) / stds\n",
    "            self.polarities = np.sum(vectors_standardized, axis=1)\n",
    "        else:\n",
    "            self.polarities = np.sum(self.vectors, axis=1)\n",
    "        \n",
    "    def calculate_corr(self, lag: int, standardize: bool = False):\n",
    "        if self.polarities is None:\n",
    "            self.calculate_polarities(standardize)\n",
    "        return np.corrcoef(self.polarities[:-lag], self.polarities[lag:])[0,1]\n",
    "        \n",
    "    def calculate_coco(self, lag: int):\n",
    "        length = self.N - lag\n",
    "        \n",
    "        U_sum = self.S[self.N - lag] - self.S[0] # sum of X[0..N-lag-1]\n",
    "        V_sum = self.S[self.N] - self.S[lag]     # sum of X[lag..N-1]\n",
    "\n",
    "        E_U = U_sum / length\n",
    "        E_V = V_sum / length\n",
    "\n",
    "        U = self.X[:self.N - lag]\n",
    "        V = self.X[lag:]\n",
    "        dot_products = np.sum(U * V, axis=1)\n",
    "        E_UV = np.mean(dot_products)\n",
    "\n",
    "        return E_UV - np.dot(E_U, E_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_plot_lrd(n, c, plot_title, **kwargs):\n",
    "    # Convert inputs to numpy arrays\n",
    "    n, c = np.array(n), np.array(c)\n",
    "\n",
    "    # Fit curves\n",
    "    popt_pl, _ = curve_fit(power_law, n, c, maxfev=5000)\n",
    "    popt_se, _ = curve_fit(stretched_exponential, n, c, bounds=([0, 0], [np.inf, 1]), maxfev=5000)\n",
    "\n",
    "    # Generate range for plotting fitted curves\n",
    "    x_fit = np.logspace(np.log10(n.min()), np.log10(n.max()), 200)\n",
    "\n",
    "    # Make the plot\n",
    "    plt.figure()\n",
    "    plt.scatter(n, c, **kwargs)\n",
    "    plt.plot(x_fit, power_law(x_fit, *popt_pl), label=\"Power law\")\n",
    "    plt.plot(x_fit, stretched_exponential(x_fit, *popt_se), label=\"Stretched exp.\")\n",
    "\n",
    "    # Decorate the plot\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(label=plot_title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [\"PG\" + str(i) for i in range(1, 10)]\n",
    "files = [\"PG1\"]\n",
    "\n",
    "for file in files:\n",
    "    file_path = f\"data/{file}_tokens.txt\"\n",
    "    vectors = load_and_embed(file_path)\n",
    "    lrd = LRDEstimator(vectors)\n",
    "\n",
    "    max_lag = int(vectors.shape[0] / 2)\n",
    "    n = range(1, max_lag)\n",
    "    c_coco = []\n",
    "    c_corr = []\n",
    "\n",
    "    for lag in n:\n",
    "        c_coco.append(lrd.calculate_coco(lag))\n",
    "        c_corr.append(lrd.calculate_corr(lag))\n",
    "\n",
    "    fit_and_plot_lrd(n, np.abs(c_coco), f\"|Coco| of {file}\", s = 1)\n",
    "    fit_and_plot_lrd(n, np.abs(c_corr), f\"|Corr| of {file}\", s = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
