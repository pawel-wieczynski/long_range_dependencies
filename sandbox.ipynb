{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External modules\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import os.path\n",
    "\n",
    "# Internal modules\n",
    "from utils import power_law, stretched_exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained *word2vec* embedding space. Below we load space for English dictionary of about ~3 mln words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_embed(file_path: str) -> np.ndarray:\n",
    "    if os.path.exists(file_path):\n",
    "        text_file = open(file_path, mode = \"r\", encoding = \"UTF-8\")\n",
    "    else:\n",
    "        raise ValueError(\"File does not exists or incorrect path provided.\")\n",
    "    \n",
    "    tokens = text_file.read().split(\"\\n\")\n",
    "    vectors = np.asarray([word2vec[w] for w in tokens if w in word2vec])\n",
    "    print(f\"Coverage: {len(vectors) / len(tokens):.4f}.\")\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy in `LRDEstimator` is to defer heavy computations until they are needed. Example of heavy computation is building prefix sums for large arrays. When it's needed then it's cached inside the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRDEstimator:\n",
    "    def __init__(self, vectors: np.ndarray):\n",
    "        self.vectors = vectors\n",
    "        self.N, self.d = vectors.shape\n",
    "\n",
    "        # Cached data\n",
    "        self.X_unpooled = None          # Normalized unpooled embeddings, shape (N, d)\n",
    "        self.S_unpooled = None          # Prefix sums of X_unpooled, shape (N+1, d)\n",
    "        self.current_pool_order = None  # Which pool_order is cached\n",
    "        self.X_pooled = None            # Normalized pooled embeddings, shape (N, d)\n",
    "        self.S_pooled = None            # Prefix sums of X_pooled, shape (N+1, d)\n",
    "\n",
    "        self.polarities = None          # For correlation-based method\n",
    "    \n",
    "    def calculate_polarities(self, standardize: bool = False):\n",
    "        if standardize:\n",
    "            means = np.mean(self.vectors, axis=1, keepdims=True)\n",
    "            stds = np.std(self.vectors, axis=1, keepdims=True)\n",
    "            stds[stds == 0] = 1.0 # Avoid division by zero - if variance is zero then calculate V - EV\n",
    "            vectors_standardized = (self.vectors - means) / stds\n",
    "            self.polarities = np.sum(vectors_standardized, axis=1)\n",
    "        else:\n",
    "            self.polarities = np.sum(self.vectors, axis=1)\n",
    "        \n",
    "    def calculate_corr(self, lag: int, standardize: bool = False):\n",
    "        if self.polarities is None:\n",
    "            self.calculate_polarities(standardize)\n",
    "        return np.corrcoef(self.polarities[:-lag], self.polarities[lag:])[0,1]\n",
    "\n",
    "    def compute_unpooled(self):\n",
    "        \"\"\"\n",
    "        Compute and cache normalized unpooled embeddings and their prefix sums if not already cached.\n",
    "        \"\"\"\n",
    "        if self.X_unpooled is None:\n",
    "            # Compute norms and normalized embeddings\n",
    "            norms = np.linalg.norm(self.vectors, axis=1, keepdims=True)\n",
    "            norms[norms == 0] = 1.0 # Avoid division by zero\n",
    "            self.X_unpooled = self.vectors / norms\n",
    "\n",
    "            # Build prefix sums\n",
    "            self.S_unpooled = np.zeros((self.N + 1, self.d))\n",
    "            self.S_unpooled[1:] = np.cumsum(self.X_unpooled, axis=0)\n",
    "    \n",
    "    def compute_pooled(self, pool_order: int):\n",
    "        \"\"\"\n",
    "        Compute and cache normalized pooled embeddings and their prefix sums if not already cached.\n",
    "        \"\"\"\n",
    "        # If we already have computed them then do nothing\n",
    "        if self.current_pool_order == pool_order and self.X_pooled is not None:\n",
    "            return\n",
    "\n",
    "        # Build prefix sums: P[i] = F[0] + F[1] + ... + F[i-1]\n",
    "        P = np.zeros((self.N + 1, self.d))\n",
    "        P[1:] = np.cumsum(self.vectors, axis=0)\n",
    "\n",
    "        # Pool embeddings\n",
    "        pooled = np.zeros((self.N, self.d))\n",
    "        for i in range(self.N):\n",
    "            m = min(i + pool_order, self.N - 1)\n",
    "            pooled[i] = P[m + 1] - P[i]\n",
    "\n",
    "        # Normalize pooled embeddings\n",
    "        pooled_norms = np.linalg.norm(pooled, axis=1, keepdims=True)\n",
    "        pooled_norms[pooled_norms == 0] = 1.0\n",
    "        self.X_pooled = pooled / pooled_norms\n",
    "\n",
    "        # Build prefix sums\n",
    "        self.S_pooled = np.zeros((self.N + 1, self.d))\n",
    "        self.S_pooled[1:] = np.cumsum(self.X_pooled, axis=0)\n",
    "\n",
    "        # Update pool order cache\n",
    "        self.current_pool_order = pool_order\n",
    "\n",
    "    def pool_embeddings(self, pool_order: int) -> np.ndarray:\n",
    "        # Build prefix sums: P[i] = F[0] + F[1] + ... + F[i-1]\n",
    "        P = np.zeros((self.N + 1, self.d))\n",
    "        P[1:] = np.cumsum(self.vectors, axis=0)\n",
    "\n",
    "        # Allocate memory for pooled embeddings\n",
    "        vectors_pooled = np.zeros((self.N, self.d))\n",
    "\n",
    "        for i in range(self.N):\n",
    "            m = min(i + pool_order, self.N - 1)\n",
    "            vectors_pooled[i] = P[m + 1] - P[i]\n",
    "        \n",
    "        return vectors_pooled\n",
    "    \n",
    "    def calculate_coco(self, lag: int, pool_order: int = 0) -> float:\n",
    "        length = self.N - lag\n",
    "        if length <= 0:\n",
    "            raise ValueError(f\"lag={lag} is too large for sequence of length {self.N}\")\n",
    "\n",
    "        if pool_order == 0:\n",
    "            # Ensure unpooled data is computed\n",
    "            self.compute_unpooled()\n",
    "\n",
    "            # Calculate CoCo on unpooled data\n",
    "            U_sum = self.S_unpooled[self.N - lag] - self.S_unpooled[0]  # sum of X[0..N-lag-1]\n",
    "            V_sum = self.S_unpooled[self.N] - self.S_unpooled[lag]      # sum of X[lag..N-1]\n",
    "\n",
    "            E_U = U_sum / length\n",
    "            E_V = V_sum / length\n",
    "\n",
    "            U = self.X_unpooled[:self.N - lag]\n",
    "            V = self.X_unpooled[lag:]\n",
    "            dot_products = np.sum(U * V, axis=1)\n",
    "            E_UV = np.mean(dot_products)\n",
    "            return E_UV - np.dot(E_U, E_V)\n",
    "\n",
    "        else:\n",
    "            # Ensure pooled data is computed\n",
    "            self.compute_pooled(pool_order)\n",
    "\n",
    "            # Calculate CoCo on pooled data\n",
    "            U_sum = self.S_pooled[self.N - lag] - self.S_pooled[0]  # sum of X[0..N-lag-1]\n",
    "            V_sum = self.S_pooled[self.N] - self.S_pooled[lag]      # sum of X[lag..N-1]\n",
    "\n",
    "            E_U = U_sum / length\n",
    "            E_V = V_sum / length\n",
    "\n",
    "            U = self.X_pooled[:self.N - lag]\n",
    "            V = self.X_pooled[lag:]\n",
    "            dot_products = np.sum(U * V, axis=1)\n",
    "            E_UV = np.mean(dot_products)\n",
    "            return E_UV - np.dot(E_U, E_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_plot_lrd(n, c, plot_title, **kwargs):\n",
    "    # Convert inputs to numpy arrays\n",
    "    n, c = np.array(n), np.array(c)\n",
    "\n",
    "    # Fit curves\n",
    "    popt_pl, _ = curve_fit(power_law, n, c, maxfev=5000)\n",
    "    popt_se, _ = curve_fit(stretched_exponential, n, c, bounds=([0, 0, -np.inf], [np.inf, 1, np.inf]), maxfev=5000)\n",
    "\n",
    "    # Generate range for plotting fitted curves\n",
    "    x_fit = np.logspace(np.log10(n.min()), np.log10(n.max()), 200)\n",
    "\n",
    "    # Make the plot\n",
    "    plt.figure()\n",
    "    plt.scatter(n, c, **kwargs)\n",
    "    plt.plot(x_fit, power_law(x_fit, *popt_pl), label=\"Power law\")\n",
    "    plt.plot(x_fit, stretched_exponential(x_fit, *popt_se), label=\"Stretched exp.\")\n",
    "\n",
    "    # Decorate the plot\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(label=plot_title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = [\"PG\" + str(i) for i in range(1, 10)]\n",
    "files = [\"PG1\"]\n",
    "\n",
    "for file in files:\n",
    "    file_path = f\"data/{file}_tokens.txt\"\n",
    "    vectors = load_and_embed(file_path)\n",
    "    lrd = LRDEstimator(vectors)\n",
    "\n",
    "    max_lag = int(vectors.shape[0] / 2)\n",
    "    n = range(1, max_lag)\n",
    "    c_coco = []\n",
    "    c_coco_8 = []\n",
    "    c_corr = []\n",
    "\n",
    "    for lag in n:\n",
    "        c_coco.append(lrd.calculate_coco(lag))\n",
    "        c_corr.append(lrd.calculate_corr(lag))\n",
    "    \n",
    "    for lag in n:\n",
    "        c_coco_8.append(lrd.calculate_coco(lag, pool_order=8))\n",
    "\n",
    "    fit_and_plot_lrd(n, np.abs(c_coco), f\"|Coco| of {file}\", s = 1)\n",
    "    fit_and_plot_lrd(n, np.abs(c_coco_8), f\"|Coco| of order 8 of {file}\", s = 1)\n",
    "    fit_and_plot_lrd(n, np.abs(c_corr), f\"|Corr| of {file}\", s = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human vs LLM Text Corpus\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/starblasters8/human-vs-llm-text-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_llm = pd.read_csv(\"data\\Human_vs_LLM_Text_Corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_llm.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_llm.value_counts(\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_vs_gpt = df_human_llm[df_human_llm[\"source\"].isin([\"Human\", \"GPT-3.5\"])]\n",
    "\n",
    "# Random sample of 100 texts per group\n",
    "df_human_vs_gpt_sample = df_human_vs_gpt.groupby(\"source\")[[\"text\", \"source\", \"word_count\"]].apply(lambda x: x.sample(20))\n",
    "\n",
    "texts_human_vs_gpt = np.asarray(df_human_vs_gpt_sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for storing the results\n",
    "coverages = []\n",
    "beta_0, beta_3, beta_8 = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(texts_human_vs_gpt):\n",
    "    # Tokenize and embed text\n",
    "    tokens = list(tokenize(text, lowercase=True))\n",
    "    vectors = np.asarray([word2vec[w] for w in tokens if w in word2vec])\n",
    "    coverages.append(len(vectors) / len(tokens))\n",
    "\n",
    "    # Initialize LRD class\n",
    "    lrd = LRDEstimator(vectors)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    c_coco_0 = []\n",
    "    c_coco_3 = []\n",
    "    c_coco_8 = []\n",
    "\n",
    "    # Calculate Coco\n",
    "    n = range(1, int(0.5 * (vectors.shape[0] - 1)))\n",
    "    for lag in n:\n",
    "        c_coco_0.append(lrd.calculate_coco(lag, pool_order=0))\n",
    "\n",
    "    # Fit power law\n",
    "    try:\n",
    "        popt_se, _ = curve_fit(stretched_exponential, n, np.abs(c_coco_0), maxfev=5000, bounds=([0, 0, -np.inf], [np.inf, 1, np.inf]))\n",
    "        beta_0.append(popt_se[1])\n",
    "    except:\n",
    "        beta_0.append(99.0)\n",
    "    \n",
    "    # Calculate Coco with 3-pooled embeddings\n",
    "    for lag in n:\n",
    "        c_coco_3.append(lrd.calculate_coco(lag, pool_order=3))\n",
    "\n",
    "    # Fit power law\n",
    "    try:\n",
    "        popt_se, _ = curve_fit(stretched_exponential, n, np.abs(c_coco_3), maxfev=5000, bounds=([0, 0, -np.inf], [np.inf, 1, np.inf]))\n",
    "        beta_3.append(popt_se[1])\n",
    "    except:\n",
    "        beta_3.append(99.0)\n",
    "    \n",
    "    # Calculate Coco with 8-pooled embeddings\n",
    "    n = range(1, int(0.5 * (vectors.shape[0] - 1)))\n",
    "    for lag in n:\n",
    "        c_coco_8.append(lrd.calculate_coco(lag, pool_order=8))\n",
    "\n",
    "    # Fit power law\n",
    "    try:\n",
    "        popt_se, _ = curve_fit(stretched_exponential, n, np.abs(c_coco_8), maxfev=5000, bounds=([0, 0, -np.inf], [np.inf, 1, np.inf]))\n",
    "        beta_8.append(popt_se[1])\n",
    "    except:\n",
    "        beta_8.append(99.0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(beta_0)\n",
    "np.mean(beta_8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
