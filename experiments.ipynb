{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99078157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import tokenize\n",
    "import LRDEstimator\n",
    "import utils\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as sps\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73232068",
   "metadata": {},
   "source": [
    "## Standardized Project Gutenberg Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cf6885",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ea384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sampled dataset\n",
    "spgc_metadata_sampled = pd.read_csv(\"data/spgc_metadata_sampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ce4a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be calculating Coco for pooled embeddings of order 1, 3, 9 and 27\n",
    "# Then we will be fitting power law (gamma) and stretched exponential function (delta and beta)\n",
    "# To our dataframe we need to add columns for storing the results\n",
    "# Additionally, power law has scaling parameter (alpha_pl) and stretched exponential has shift parameters (alpha_se)\n",
    "# After fitting parameters we will calculate error metric\n",
    "new_columns = [\n",
    "    # Power law parameters\n",
    "    \"gamma_1\", \"gamma_3\", \"gamma_9\", \"gamma_27\",\n",
    "    \"alpha_pl_1\", \"alpha_pl_3\", \"alpha_pl_9\", \"alpha_pl_27\",\n",
    "    \"error_pl_1\", \"error_pl_3\", \"error_pl_9\", \"error_pl_27\",\n",
    "\n",
    "    # Stretched exponential parameters\n",
    "    \"delta_1\", \"delta_3\", \"delta_9\", \"delta_27\",\n",
    "    \"beta_1\", \"beta_3\", \"beta_9\", \"beta_27\",\n",
    "    \"alpha_se_1\", \"alpha_se_3\", \"alpha_se_9\", \"alpha_se_27\",\n",
    "    \"error_se_1\", \"error_se_3\", \"error_se_9\", \"error_se_27\",\n",
    "]\n",
    "spgc_metadata_sampled[new_columns] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c40e879",
   "metadata": {},
   "source": [
    "### Define reusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec_embeddings(language_code: str, base_path: str = \"embeddings\") -> KeyedVectors:\n",
    "    \"\"\"\n",
    "    Load word2vec embeddings for a given language code.\n",
    "    \"\"\"\n",
    "    model_path = f\"{base_path}/word2vec_{language_code}.bin\"\n",
    "    return KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(file_name: str, base_path: str = \"data/SGPC\") -> list:\n",
    "    \"\"\"\n",
    "    Load tokens from a file and return them as a list.\n",
    "    \"\"\"\n",
    "    file_path = f\"{base_path}/{file_name}_tokens.txt\"\n",
    "    with open(file_path, mode=\"r\", encoding=\"UTF-8\") as f:\n",
    "        tokens = f.read().split(\"\\n\")\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce296431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coco_values_for_book(vectors: np.ndarray, \n",
    "                                 file_name: str,\n",
    "                                 language: str,\n",
    "                                 pool_order: int,\n",
    "                                 coco_results: list,\n",
    "                                 lag_growth_factor: float = 1.1):\n",
    "    \"\"\"\n",
    "    Given the vectors for a single book, compute CoCo values at various lags\n",
    "    and store them in the coco_results list of dicts.\n",
    "    \"\"\"\n",
    "    lrd = LRDEstimator.LRDEstimator(vectors)\n",
    "    max_lag = int(vectors.shape[0] / 2) # TBD: maybe this should be smaller for poolend embeddings?\n",
    "    current_lag = 1\n",
    "    \n",
    "    while current_lag < max_lag:\n",
    "        # Example without permutation test:\n",
    "        coco_value = lrd.calculate_coco(lag=current_lag, pool_order=pool_order)\n",
    "        \n",
    "        coco_results.append({\n",
    "            \"language\": language,\n",
    "            \"book_id\": file_name,\n",
    "            \"pool_order\": pool_order,\n",
    "            \"lag\": int(current_lag),\n",
    "            \"coco_value\": coco_value,\n",
    "        })\n",
    "        \n",
    "        # Increase lag by ~10%\n",
    "        current_lag = int(np.ceil(current_lag * lag_growth_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_store_curves(spgc_df, coco_results, file_name, pool_order):\n",
    "    \"\"\"\n",
    "    Fit power law and stretched exponential to the CoCo results for a single\n",
    "    book_id and pool_order. Store results back into spgc_df.\n",
    "    \"\"\"\n",
    "    # Filter the relevant CoCo results\n",
    "    book_data = [\n",
    "        r for r in coco_results \n",
    "        if (r[\"book_id\"] == file_name) and (r[\"pool_order\"] == pool_order)\n",
    "    ]\n",
    "    if not book_data:\n",
    "        return  # No data to fit\n",
    "    \n",
    "    lags = np.array([d[\"lag\"] for d in book_data])\n",
    "    c_coco = np.array([d[\"coco_value\"] for d in book_data])\n",
    "    abs_coco = np.abs(c_coco)\n",
    "    \n",
    "    # Fit power law\n",
    "    try:\n",
    "        popt_pl, _ = curve_fit(\n",
    "            utils.power_law,\n",
    "            lags,\n",
    "            abs_coco,\n",
    "            bounds=([-np.inf, -np.inf], [np.inf, 0]),\n",
    "            maxfev=5000\n",
    "        )\n",
    "        alpha_col = f\"alpha_pl_{pool_order}\"\n",
    "        gamma_col = f\"gamma_{pool_order}\"\n",
    "        spgc_df.loc[spgc_df[\"id\"] == file_name, alpha_col] = popt_pl[0]\n",
    "        spgc_df.loc[spgc_df[\"id\"] == file_name, gamma_col] = popt_pl[1]\n",
    "\n",
    "        # Calculate the fitted values\n",
    "        fitted_values = utils.power_law(lags, *popt_pl)\n",
    "        # Calculate the error\n",
    "        error = utils.calculate_wssr(abs_coco, fitted_values)\n",
    "        error_col = f\"error_pl_{pool_order}\"\n",
    "        spgc_df.loc[spgc_df[\"id\"] == file_name, error_col] = error\n",
    "        \n",
    "    except RuntimeError:\n",
    "        pass  # If fitting fails, leave as NaN\n",
    "    \n",
    "    # Fit stretched exponential\n",
    "    try:\n",
    "        popt_se, _ = curve_fit(\n",
    "            utils.stretched_exponential,\n",
    "            lags,\n",
    "            abs_coco,\n",
    "            bounds=([0, 0, -np.inf], [np.inf, 1, np.inf]),\n",
    "            maxfev=5000\n",
    "        )\n",
    "        delta_col = f\"delta_{pool_order}\"\n",
    "        beta_col = f\"beta_{pool_order}\"\n",
    "        alpha_col = f\"alpha_se_{pool_order}\"\n",
    "        spgc_df.loc[spgc_df[\"id\"] == file_name, delta_col] = popt_se[0]\n",
    "        spgc_df.loc[spgc_df[\"id\"] == file_name, beta_col] = popt_se[1]\n",
    "        spgc_df.loc[spgc_df[\"id\"] == file_name, alpha_col] = popt_se[2]\n",
    "\n",
    "        # Calculate the fitted values\n",
    "        fitted_values = utils.stretched_exponential(lags, *popt_se)\n",
    "        # Calculate the error\n",
    "        error = utils.calculate_wssr(abs_coco, fitted_values)\n",
    "        error_col = f\"error_se_{pool_order}\"\n",
    "        spgc_df.loc[spgc_df[\"id\"] == file_name, error_col] = error\n",
    "    \n",
    "    except RuntimeError:\n",
    "        pass  # If fitting fails, leave as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3731b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_language(spgc_df, language, pool_orders, coco_results):\n",
    "    \"\"\"\n",
    "    Process all books for a single language and multiple pool_orders.\n",
    "    \"\"\"\n",
    "    print(f\"Processing language: {language}\")\n",
    "    language_code = re.findall(r\"[a-z]{2}\", language)[0]\n",
    "    \n",
    "    # Filter metadata for the current language\n",
    "    spgc_metadata_current = spgc_df[spgc_df[\"language\"] == language]\n",
    "    \n",
    "    # Load embeddings once per language\n",
    "    model_current = load_word2vec_embeddings(language_code)\n",
    "    \n",
    "    # Iterate through each book in the current language\n",
    "    for index, row in spgc_metadata_current.iterrows():\n",
    "        file_name = row[\"id\"]\n",
    "        \n",
    "        # Load tokens\n",
    "        tokens = load_tokens(file_name)\n",
    "        \n",
    "        # Build embeddings\n",
    "        vectors = np.asarray([model_current[w] for w in tokens if w in model_current])\n",
    "        \n",
    "        # Skip if not enough vectors\n",
    "        if len(vectors) < 2:\n",
    "            continue\n",
    "        \n",
    "        # For each pool_order, compute CoCo values\n",
    "        for p_order in pool_orders:\n",
    "            compute_coco_values_for_book(\n",
    "                vectors=vectors,\n",
    "                file_name=file_name,\n",
    "                language=language,\n",
    "                pool_order=p_order,\n",
    "                coco_results=coco_results\n",
    "            )\n",
    "        \n",
    "        # For each pool_order, fit the curves and store\n",
    "        for p_order in pool_orders:\n",
    "            fit_and_store_curves(spgc_df, coco_results, file_name, p_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a35b6c",
   "metadata": {},
   "source": [
    "### Run calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_coco_pipeline(spgc_metadata_sampled):\n",
    "    \"\"\"\n",
    "    Main pipeline function that:\n",
    "      1) Determines unique languages,\n",
    "      2) Iterates over each language,\n",
    "      3) Computes CoCo for each pool_order,\n",
    "      4) Fits curves, and\n",
    "      5) Stores results.\n",
    "    \"\"\"\n",
    "    languages = spgc_metadata_sampled[\"language\"].unique()\n",
    "    pool_orders = [0, 3, 9, 27]\n",
    "    \n",
    "    # We'll collect results in this list of dictionaries\n",
    "    coco_results_records = []\n",
    "    \n",
    "    for language in languages:\n",
    "        process_language(\n",
    "            spgc_df=spgc_metadata_sampled,\n",
    "            language=language,\n",
    "            pool_orders=pool_orders,\n",
    "            coco_results=coco_results_records\n",
    "        )\n",
    "    \n",
    "    return spgc_metadata_sampled, coco_results_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac19821",
   "metadata": {},
   "outputs": [],
   "source": [
    "spgc_metadata_sampled = pd.read_csv(\"data/spgc_metadata_sampled.csv\")\n",
    "spgc_metadata_sampled, coco_results = main_coco_pipeline(spgc_metadata_sampled)\n",
    "\n",
    "# It takes ~2 hours to complete\n",
    "\n",
    "# Save the results to a CSV file\n",
    "spgc_metadata_sampled.to_csv(\"results/spgc_metadata_sampled_after.csv\", index=False)\n",
    "coco_results_df = pd.DataFrame(coco_results)\n",
    "coco_results_df.to_csv(\"results/coco_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74456364",
   "metadata": {},
   "source": [
    "## Human vs LLM Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251b381",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sampled dataset\n",
    "df_human_vs_llm_sampled = pd.read_csv(\"data/human_vs_llm_sampled.csv\")\n",
    "\n",
    "new_columns = [\n",
    "    # Power law parameters\n",
    "    \"gamma_1\", \"gamma_3\", \"gamma_9\", \"gamma_27\",\n",
    "    \"alpha_pl_1\", \"alpha_pl_3\", \"alpha_pl_9\", \"alpha_pl_27\",\n",
    "    \"error_pl_1\", \"error_pl_3\", \"error_pl_9\", \"error_pl_27\",\n",
    "\n",
    "    # Stretched exponential parameters\n",
    "    \"delta_1\", \"delta_3\", \"delta_9\", \"delta_27\",\n",
    "    \"beta_1\", \"beta_3\", \"beta_9\", \"beta_27\",\n",
    "    \"alpha_se_1\", \"alpha_se_3\", \"alpha_se_9\", \"alpha_se_27\",\n",
    "    \"error_se_1\", \"error_se_3\", \"error_se_9\", \"error_se_27\",\n",
    "]\n",
    "df_human_vs_llm_sampled[new_columns] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"embeddings/word2vec_en.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d978d",
   "metadata": {},
   "source": [
    "### Run calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_orders = [1, 3, 9, 27]\n",
    "coco_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78465b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Runtime approx. 3h\n",
    "for i, row in df_human_vs_llm_sampled.iterrows():\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"Processing row {i+1}/{len(df_human_vs_llm_sampled)}\")\n",
    "    text = row[\"text\"]\n",
    "    tokens = list(tokenize(text, lowercase=True))\n",
    "    vectors = np.asarray([model[w] for w in tokens if w in model])\n",
    "    \n",
    "    # Skip if not enough vectors\n",
    "    if len(vectors) < 2:\n",
    "        continue\n",
    "    \n",
    "    # For each pool_order, compute CoCo values\n",
    "    for p_order in pool_orders:\n",
    "        lrd = LRDEstimator.LRDEstimator(vectors)\n",
    "        max_lag = int(vectors.shape[0] / 2)\n",
    "        current_lag = 1\n",
    "        while current_lag < max_lag:\n",
    "            coco_value = lrd.calculate_coco(lag=current_lag, pool_order=p_order)\n",
    "            coco_results.append({\n",
    "                \"text\": text,\n",
    "                \"source\": row[\"source\"],\n",
    "                \"pool_order\": p_order,\n",
    "                \"lag\": int(current_lag),\n",
    "                \"coco_value\": coco_value,\n",
    "            })\n",
    "            current_lag = int(np.ceil(current_lag * 1.1))\n",
    "        \n",
    "    \n",
    "    # For each pool_order, fit the curves\n",
    "    for p_order in pool_orders:\n",
    "        # Filter the relevant CoCo results\n",
    "        book_data = [\n",
    "            r for r in coco_results \n",
    "            if (r[\"text\"] == text) and (r[\"pool_order\"] == p_order)\n",
    "        ]\n",
    "        if not book_data:\n",
    "            continue\n",
    "        \n",
    "        lags = np.array([d[\"lag\"] for d in book_data])\n",
    "        c_coco = np.array([d[\"coco_value\"] for d in book_data])\n",
    "        abs_coco = np.abs(c_coco)\n",
    "        \n",
    "        # Fit power law\n",
    "        try:\n",
    "            popt_pl, _ = curve_fit(\n",
    "                utils.power_law,\n",
    "                lags,\n",
    "                abs_coco,\n",
    "                bounds=([-np.inf, -np.inf], [np.inf, 0]),\n",
    "                maxfev=5000\n",
    "            )\n",
    "            alpha_col = f\"alpha_pl_{p_order}\"\n",
    "            gamma_col = f\"gamma_{p_order}\"\n",
    "            df_human_vs_llm_sampled.loc[i, alpha_col] = popt_pl[0]\n",
    "            df_human_vs_llm_sampled.loc[i, gamma_col] = popt_pl[1]\n",
    "\n",
    "            # Calculate the fitted values\n",
    "            fitted_values = utils.power_law(lags, *popt_pl)\n",
    "            # Calculate the error\n",
    "            error = utils.calculate_wssr(abs_coco, fitted_values)\n",
    "            error_col = f\"error_pl_{p_order}\"\n",
    "            df_human_vs_llm_sampled.loc[i, error_col] = error\n",
    "\n",
    "        except RuntimeError:\n",
    "            pass  # If fitting fails, leave as NaN\n",
    "        \n",
    "        # Fit stretched exponential\n",
    "        try:\n",
    "            popt_se, _ = curve_fit(\n",
    "                utils.stretched_exponential,\n",
    "                lags,\n",
    "                abs_coco,\n",
    "                bounds=([0, 0, -np.inf], [np.inf, 1, np.inf]),\n",
    "                maxfev=5000\n",
    "            )\n",
    "            delta_col = f\"delta_{p_order}\"\n",
    "            beta_col = f\"beta_{p_order}\"\n",
    "            alpha_col = f\"alpha_se_{p_order}\"\n",
    "            df_human_vs_llm_sampled.loc[i, delta_col] = popt_se[0]\n",
    "            df_human_vs_llm_sampled.loc[i, beta_col] = popt_se[1]\n",
    "            df_human_vs_llm_sampled.loc[i, alpha_col] = popt_se[2]\n",
    "\n",
    "            # Calculate the fitted values\n",
    "            fitted_values = utils.stretched_exponential(lags, *popt_se)\n",
    "            # Calculate the error\n",
    "            error = utils.calculate_wssr(abs_coco, fitted_values)\n",
    "            error_col = f\"error_se_{p_order}\"\n",
    "            df_human_vs_llm_sampled.loc[i, error_col] = error\n",
    "            \n",
    "        except RuntimeError:\n",
    "            pass  # If fitting fails, leave as NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ffbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "df_human_vs_llm_sampled.to_csv(\"results/human_vs_llm_sampled_after.csv\", index=False)\n",
    "coco_results_df = pd.DataFrame(coco_results)\n",
    "coco_results_df.to_csv(\"results/human_vs_llm_coco_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
