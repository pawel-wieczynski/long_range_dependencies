{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b26c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea7246",
   "metadata": {},
   "source": [
    "## Standardized Project Gutenberg Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a86a5a",
   "metadata": {},
   "source": [
    "### Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the metadata file\n",
    "spgc_metadata = pd.read_csv(r\"data\\SPGC-metadata-2018-07-18.csv\")\n",
    "\n",
    "# Due to computational resources select only texts with size less than 1000 KB\n",
    "spgc_metadata = spgc_metadata[spgc_metadata[\"file_size\"] < 1000]\n",
    "\n",
    "# Count the number of texts per language\n",
    "spgc_languages_count = spgc_metadata.value_counts(\"language\")\n",
    "\n",
    "# Select the top 20 languages\n",
    "spgc_languages_chosen = spgc_languages_count.head(20).index.tolist()\n",
    "\n",
    "# Skip languages for which embeddings are not available in NLPL repository\n",
    "spgc_languages_chosen.remove(\"['eo']\") # Esperanto\n",
    "spgc_languages_chosen.remove(\"['tl']\") # Tagalog\n",
    "\n",
    "# Skip language which had low coverage during the calculations\n",
    "spgc_languages_chosen.remove(\"['zh']\") # Chinese\n",
    "\n",
    "# Filter the metadata to include only the chosen languages\n",
    "spgc_metadata_filtered = spgc_metadata[spgc_metadata[\"language\"].isin(spgc_languages_chosen)]\n",
    "\n",
    "# Sample 100 texts per language\n",
    "# If a language has less than 100 texts, sample all of them\n",
    "spgc_metadata_sampled = (\n",
    "    spgc_metadata_filtered.groupby(\"language\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=min(len(x), 100), random_state=213))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc130e8f",
   "metadata": {},
   "source": [
    "### Count number of tokens and coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in spgc_languages_chosen:\n",
    "    print(f\"Processing language: {language}\")\n",
    "    # Filter the metadata for the current language\n",
    "    spgc_metadata_language = spgc_metadata_sampled[spgc_metadata_sampled[\"language\"] == language]\n",
    "\n",
    "    # Load embeddings for the current language\n",
    "    language_code = re.findall(r\"[a-z]{2}\", language)[0]\n",
    "    model_current = KeyedVectors.load_word2vec_format(f\"embeddings\\word2vec_{language_code}.bin\", binary=True)\n",
    "\n",
    "    # Iterate over all texts in the current language\n",
    "    for index, row in spgc_metadata_language.iterrows():\n",
    "        file_name = row[\"id\"] + \"_tokens.txt\"\n",
    "        file_path = f\"data/SGPC/{file_name}\"\n",
    "\n",
    "        # Read the text file\n",
    "        text_file = open(file_path, mode=\"r\", encoding=\"UTF-8\")\n",
    "        tokens = text_file.read().split(\"\\n\")\n",
    "        text_file.close()\n",
    "\n",
    "        # Count the number of tokens in the text\n",
    "        token_count = len(tokens)\n",
    "        spgc_metadata_sampled.at[index, \"token_count\"] = token_count\n",
    "\n",
    "        # Embed tokens\n",
    "        vectors = np.asarray([model_current[w] for w in tokens if w in model_current])\n",
    "\n",
    "        # Calculate coverage\n",
    "        coverage = len(vectors) / token_count if token_count > 0 else 0\n",
    "        spgc_metadata_sampled.at[index, \"coverage\"] = coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95106660",
   "metadata": {},
   "outputs": [],
   "source": [
    "spgc_metadata_sampled.to_csv(\"data/spgc_metadata_sampled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f37d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "spgc_metadata_sampled = pd.read_csv(\"data/spgc_metadata_sampled.csv\")\n",
    "# Remove Chinese\n",
    "spgc_metadata_sampled = spgc_metadata_sampled[spgc_metadata_sampled[\"language\"] != \"['zh']\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fff586",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8486ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spgc_languages_stats_tokens = spgc_metadata_sampled.groupby(\"language\")[\"token_count\"].describe().loc[:, ['count', 'min', 'max', 'mean', 'std']]\n",
    "print(spgc_languages_stats_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98414686",
   "metadata": {},
   "outputs": [],
   "source": [
    "spgc_languages_stats_coverage = spgc_metadata_sampled.groupby(\"language\")[\"coverage\"].describe().loc[:, ['count', 'min', 'max', 'mean', 'std']]\n",
    "print(spgc_languages_stats_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d44825",
   "metadata": {},
   "source": [
    "## Human vs LLM Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All texts are in English\n",
    "model = KeyedVectors.load_word2vec_format(\"embeddings/word2vec_en.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285dab0",
   "metadata": {},
   "source": [
    "### Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557803c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the human vs LLM text corpus\n",
    "df_human_vs_llm = pd.read_csv(\"data\\Human_vs_LLM_Text_Corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaca2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select few sources for the analysis\n",
    "sources_chosen = [\"Human\", \"GPT-3.5\", \"GPT-4\", \"LLaMA-7B\", \"LLaMA-13B\", \"LLaMA-30B\", \"LLaMA-65B\"]\n",
    "df_human_vs_llm = df_human_vs_llm[df_human_vs_llm[\"source\"].isin(sources_chosen)]\n",
    "\n",
    "# Sample 1000 texts per source\n",
    "df_human_vs_llm_sampled = (\n",
    "    df_human_vs_llm.groupby(\"source\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=min(len(x), 1000), random_state=213))\n",
    ")\n",
    "\n",
    "df_human_vs_llm_sampled.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ac2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_vs_llm_sampled.to_csv(\"data/human_vs_llm_sampled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_vs_llm_sampled = pd.read_csv(\"data/human_vs_llm_sampled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bfbace",
   "metadata": {},
   "source": [
    "### Count number of tokens and coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e01e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_human_vs_llm_sampled.iterrows():\n",
    "    text = row[\"text\"]\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = list(tokenize(text, lowercase=True))\n",
    "\n",
    "    # Count the number of tokens in the text\n",
    "    token_count = len(tokens)\n",
    "    df_human_vs_llm_sampled.at[index, \"token_count\"] = token_count\n",
    "\n",
    "    # Embed tokens\n",
    "    vectors = np.asarray([model[w] for w in tokens if w in model])\n",
    "\n",
    "    # Calculate coverage\n",
    "    coverage = len(vectors) / token_count if token_count > 0 else 0\n",
    "    df_human_vs_llm_sampled.at[index, \"coverage\"] = coverage\n",
    "\n",
    "    # Print progress every 1000 texts\n",
    "    if (index+1) % 1000 == 0:\n",
    "        print(f\"Processed {index+1} texts...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66f61a",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd079cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_vs_llm_stats_tokens = df_human_vs_llm_sampled.groupby(\"source\")[\"token_count\"].describe().loc[:, ['count', 'min', 'max', 'mean', 'std']]\n",
    "print(human_vs_llm_stats_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_vs_llm_stats_coverage = df_human_vs_llm_sampled.groupby(\"source\")[\"coverage\"].describe().loc[:, ['count', 'min', 'max', 'mean', 'std']]\n",
    "print(human_vs_llm_stats_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250cc42d",
   "metadata": {},
   "source": [
    "## To LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spgc_languages_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f61fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_dictionary = {\n",
    "    \"['en']\": \"English\",\n",
    "    \"['fr']\": \"French\",\n",
    "    \"['fi']\": \"Finnish\", \n",
    "    \"['de']\": \"German\", \n",
    "    \"['nl']\": \"Dutch\", \n",
    "    \"['it']\": \"Italian\", \n",
    "    \"['es']\": \"Spanish\", \n",
    "    \"['pt']\": \"Portuguese\", \n",
    "    # \"['zh']\": \"Chinese\", \n",
    "    \"['el']\": \"Greek\", \n",
    "    \"['sv']\": \"Swedish\", \n",
    "    \"['hu']\": \"Hungarian\", \n",
    "    \"['la']\": \"Latin\",\n",
    "    \"['da']\": \"Danish\",\n",
    "    \"['ca']\": \"Catalan\",\n",
    "    \"['pl']\": \"Polish\",\n",
    "    \"['ja']\": \"Japanese\",\n",
    "    \"['no']\": \"Norwegian\",\n",
    "}\n",
    "\n",
    "spgc_languages_stats_tokens.index = spgc_languages_stats_tokens.index.map(languages_dictionary)\n",
    "spgc_languages_stats_coverage.index = spgc_languages_stats_coverage.index.map(languages_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Count\", \"Mean\", \"St.D.\", \"Min\", \"Q1\", \"Q2\", \"Q3\", \"Max\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_spgc_languages_stats_tokens = spgc_languages_stats_tokens.to_latex(\n",
    "    # float_format=\"%.1f\",\n",
    "    # header=headers,\n",
    "    # label=\"tab:spgc_languages_stats_tokens\",\n",
    "    # caption=\"Statistics of token count for the sampled SPGC texts in different languages.\",\n",
    ")\n",
    "print(latex_spgc_languages_stats_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878dc11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_spgc_languages_stats_coverage = spgc_languages_stats_coverage.to_latex(\n",
    "    # float_format=\"%.1f\",\n",
    "    # header=headers,\n",
    "    # label=\"tab:spgc_languages_stats_coverage\",\n",
    "    # caption=\"Statistics of coverage of tokens by NLPL embeddings for the sampled SPGC texts in different languages.\",\n",
    ")\n",
    "print(latex_spgc_languages_stats_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2319cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_human_vs_llm_stats_tokens = human_vs_llm_stats_tokens.to_latex(\n",
    "    # float_format=\"%.1f\",\n",
    "    # header=headers,\n",
    "    # label=\"tab:human_ai_stats_counts\",\n",
    "    # caption=\"Statistics of token count for the sampled Human vs LLM texts.\"\n",
    ")\n",
    "print(latex_human_vs_llm_stats_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b63e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_human_vs_llm_stats_coverage = human_vs_llm_stats_coverage.to_latex(\n",
    "    # float_format=\"%.1f\",\n",
    "    # header=headers,\n",
    "    # label=\"tab:human_ai_stats_coverage\",\n",
    "    # caption=\"Statistics of coverage of tokens by NLPL embeddings for the sampled Human vs LLM texts.\"\n",
    ")\n",
    "print(latex_human_vs_llm_stats_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaede6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table with properties of embedding spaces\n",
    "nlpl_properties = pd.read_csv(\"data/embeddings_space_sizes.csv\")\n",
    "nlpl_properties_latex = nlpl_properties.to_latex(\n",
    "    float_format=\"%.0f\",\n",
    "    label=\"tab:embeddings_space_sizes\",\n",
    "    caption=\"Properties of the embedding spaces for different languages.\",\n",
    ")\n",
    "print(nlpl_properties_latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
